# üéØ **CLARIFICATION : APPROCHE ACTUELLE VS APPROCHE RECOMMAND√âE**

Excellente question ! Laissez-moi vous expliquer la diff√©rence entre ce que vous avez fait et ce que votre coll√®gue recommande.

---

## üìä **VOTRE APPROCHE ACTUELLE**

```yaml
waxng_job:
  tasks:
    - bdd-tests         # Tests inline
    - unzip
    - auto-loader
    - waxng-ingestion
```

**Probl√®mes :**
- ‚ùå Tests m√©lang√©s avec le code de production
- ‚ùå Un seul dataset √† la fois
- ‚ùå Pas de comparaison baseline
- ‚ùå Tests bloquent le pipeline si √©chec

---

## üöÄ **APPROCHE RECOMMAND√âE PAR VOTRE COLL√àGUE**

### **Architecture compl√®te :**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    JOBS DE PRODUCTION                        ‚îÇ
‚îÇ                   (R√©utilisables)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Job 1: waxng_pipeline_site
‚îú‚îÄ‚îÄ unzip (site)
‚îú‚îÄ‚îÄ auto-loader (site)
‚îî‚îÄ‚îÄ waxng-ingestion (site)

Job 2: waxng_pipeline_activite
‚îú‚îÄ‚îÄ unzip (activite)
‚îú‚îÄ‚îÄ auto-loader (activite)
‚îî‚îÄ‚îÄ waxng-ingestion (activite)

Job 3: waxng_pipeline_clymene
‚îú‚îÄ‚îÄ unzip (clymene)
‚îú‚îÄ‚îÄ auto-loader (clymene)
‚îî‚îÄ‚îÄ waxng-ingestion (clymene)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              JOB ORCHESTRATEUR DE TESTS                      ‚îÇ
‚îÇ                    (S√©par√©)                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Job Test Orchestrator
‚îú‚îÄ‚îÄ 1. Lance Job 1 (site)
‚îú‚îÄ‚îÄ 2. Lance Job 2 (activite)
‚îú‚îÄ‚îÄ 3. Lance Job 3 (clymene)
‚îú‚îÄ‚îÄ 4. Attendre fin de tous les jobs
‚îú‚îÄ‚îÄ 5. R√©cup√©rer outputs (tables Delta)
‚îú‚îÄ‚îÄ 6. Charger baseline de r√©f√©rence
‚îú‚îÄ‚îÄ 7. Comparer r√©sultats vs baseline
‚îÇ      ‚îú‚îÄ‚îÄ Nombre de lignes
‚îÇ      ‚îú‚îÄ‚îÄ Qualit√© des donn√©es
‚îÇ      ‚îú‚îÄ‚îÄ Colonnes pr√©sentes
‚îÇ      ‚îî‚îÄ‚îÄ Valeurs NULL
‚îî‚îÄ‚îÄ 8. G√©n√©rer rapport consolid√©
```

---

## üìù **NOUVELLE STRUCTURE COMPL√àTE**

### **1. `resources/waxng_job.yml` (JOBS DE PRODUCTION)**

```yaml
# resources/waxng_job.yml

resources:
  jobs:
    # ==========================================
    # JOB 1 : PIPELINE SITE (Production)
    # ==========================================
    waxng_pipeline_site:
      name: waxng_pipeline_site
      
      parameters:
        - name: zip_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/"
        - name: excel_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/config/"
        - name: extract_dir
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/preprocessed/"
        - name: dataset
          default: "site"
      
      tasks:
        # Task 1 : D√©compression
        - task_key: unzip
          description: "Extraction ZIP - SITE"
          python_wheel_task:
            package_name: waxng
            entry_point: unzip_module
            parameters:
              - "--zip_path={{job.parameters.zip_path}}"
              - "--extract_dir={{job.parameters.extract_dir}}"
              - "--dataset={{job.parameters.dataset}}"
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          libraries:
            - whl: ../dist/waxng-1.0.0-py3-none-any.whl
        
        # Task 2 : Auto Loader
        - task_key: auto-loader
          depends_on:
            - task_key: unzip
          description: "Auto Loader - SITE"
          python_wheel_task:
            package_name: waxng
            entry_point: autoloader_module
            parameters:
              - "--source_path={{job.parameters.extract_dir}}"
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
        
        # Task 3 : Ingestion finale
        - task_key: waxng-ingestion
          depends_on:
            - task_key: auto-loader
          description: "Ingestion finale - SITE"
          python_wheel_task:
            package_name: waxng
            entry_point: main
            parameters:
              - "--dataset={{job.parameters.dataset}}"
              - "--excel_path={{job.parameters.excel_path}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
    
    # ==========================================
    # JOB 2 : PIPELINE ACTIVITE (Production)
    # ==========================================
    waxng_pipeline_activite:
      name: waxng_pipeline_activite
      
      parameters:
        - name: dataset
          default: "activite"
        - name: zip_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/"
        - name: extract_dir
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/preprocessed/"
        - name: excel_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/config/"
      
      tasks:
        - task_key: unzip
          description: "Extraction ZIP - ACTIVITE"
          python_wheel_task:
            package_name: waxng
            entry_point: unzip_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          libraries:
            - whl: ../dist/waxng-1.0.0-py3-none-any.whl
        
        - task_key: auto-loader
          depends_on:
            - task_key: unzip
          description: "Auto Loader - ACTIVITE"
          python_wheel_task:
            package_name: waxng
            entry_point: autoloader_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
        
        - task_key: waxng-ingestion
          depends_on:
            - task_key: auto-loader
          description: "Ingestion finale - ACTIVITE"
          python_wheel_task:
            package_name: waxng
            entry_point: main
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
    
    # ==========================================
    # JOB 3 : PIPELINE CLYMENE (Production)
    # ==========================================
    waxng_pipeline_clymene:
      name: waxng_pipeline_clymene
      
      parameters:
        - name: dataset
          default: "clymene"
        - name: zip_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/"
        - name: extract_dir
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/preprocessed/"
        - name: excel_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/config/"
      
      tasks:
        - task_key: unzip
          description: "Extraction ZIP - CLYMENE"
          python_wheel_task:
            package_name: waxng
            entry_point: unzip_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          libraries:
            - whl: ../dist/waxng-1.0.0-py3-none-any.whl
        
        - task_key: auto-loader
          depends_on:
            - task_key: unzip
          description: "Auto Loader - CLYMENE"
          python_wheel_task:
            package_name: waxng
            entry_point: autoloader_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
        
        - task_key: waxng-ingestion
          depends_on:
            - task_key: auto-loader
          description: "Ingestion finale - CLYMENE"
          python_wheel_task:
            package_name: waxng
            entry_point: main
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
```

---

### **2. `resources/waxng_test_orchestrator.yml` (JOB DE TEST S√âPAR√â)**

```yaml
# resources/waxng_test_orchestrator.yml

resources:
  jobs:
    # ==========================================
    # JOB ORCHESTRATEUR DE TESTS
    # ==========================================
    waxng_test_orchestrator:
      name: waxng_test_orchestrator
      description: "Orchestre les tests E2E sur tous les datasets"
      
      parameters:
        - name: baseline_path
          default: "s3://baseline/wax/"
        - name: generate_report
          default: "true"
      
      tasks:
        # ==========================================
        # TASK 0 : Ex√©cuter les tests BDD
        # ==========================================
        - task_key: run-bdd-tests
          description: "Orchestrateur de tests BDD avec baseline"
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/waxng/notebooks/test_orchestrator
            base_parameters:
              baseline_path: "{{job.parameters.baseline_path}}"
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          libraries:
            - whl: ../dist/waxng-1.0.0-py3-none-any.whl
            - pypi:
                package: behave>=1.2.6
            - pypi:
                package: databricks-sdk
```

---

### **3. `notebooks/test_orchestrator.py` (ORCHESTRATEUR)**

```python
# Databricks notebook source

# MAGIC %md
# MAGIC # üéØ Orchestrateur de Tests E2E WAX NG
# MAGIC 
# MAGIC Ce notebook :
# MAGIC 1. Lance les 3 pipelines de production
# MAGIC 2. R√©cup√®re leurs outputs
# MAGIC 3. Compare avec baseline
# MAGIC 4. G√©n√®re un rapport consolid√©

# COMMAND ----------

# MAGIC %md
# MAGIC ## üì¶ Installation

# COMMAND ----------

%pip install behave>=1.2.6 databricks-sdk --quiet
dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ## üé¨ Configuration

# COMMAND ----------

from databricks.sdk import WorkspaceClient
from databricks.sdk.service import jobs
import time
from datetime import datetime

# Configuration
BASELINE_PATH = dbutils.widgets.get("baseline_path")

w = WorkspaceClient()

# Jobs √† lancer
JOBS_TO_RUN = [
    "waxng_pipeline_site",
    "waxng_pipeline_activite",
    "waxng_pipeline_clymene"
]

print(f"üéØ Baseline: {BASELINE_PATH}")
print(f"üéØ Jobs √† tester: {', '.join(JOBS_TO_RUN)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üöÄ √âtape 1 : Lancer les jobs de production

# COMMAND ----------

job_runs = {}

for job_name in JOBS_TO_RUN:
    print(f"\nüöÄ Lancement du job: {job_name}")
    
    # Trouver le job
    job_list = list(w.jobs.list(name=job_name))
    
    if not job_list:
        raise Exception(f"‚ùå Job non trouv√©: {job_name}")
    
    job = job_list[0]
    
    # Lancer le job
    run = w.jobs.run_now(job_id=job.job_id)
    
    job_runs[job_name] = {
        'job_id': job.job_id,
        'run_id': run.run_id,
        'status': 'RUNNING',
        'start_time': datetime.now()
    }
    
    print(f"   ‚úÖ Lanc√© - Run ID: {run.run_id}")

print(f"\n‚úÖ {len(job_runs)} jobs lanc√©s")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚è≥ √âtape 2 : Attendre la fin de tous les jobs

# COMMAND ----------

import time

MAX_WAIT_MINUTES = 60
CHECK_INTERVAL_SECONDS = 30

start_time = time.time()
max_wait_seconds = MAX_WAIT_MINUTES * 60

print(f"‚è≥ Attente de la fin des jobs (max {MAX_WAIT_MINUTES} min)...\n")

while time.time() - start_time < max_wait_seconds:
    all_done = True
    
    for job_name, run_info in job_runs.items():
        if run_info['status'] in ['SUCCESS', 'FAILED']:
            continue
        
        # V√©rifier le statut
        run = w.jobs.get_run(run_id=run_info['run_id'])
        
        if run.state.life_cycle_state == 'TERMINATED':
            if run.state.result_state == 'SUCCESS':
                run_info['status'] = 'SUCCESS'
                print(f"‚úÖ {job_name}: SUCCESS")
            else:
                run_info['status'] = 'FAILED'
                print(f"‚ùå {job_name}: FAILED - {run.state.state_message}")
        else:
            all_done = False
    
    if all_done:
        break
    
    time.sleep(CHECK_INTERVAL_SECONDS)

# V√©rifier si tous ont r√©ussi
failed_jobs = [name for name, info in job_runs.items() if info['status'] != 'SUCCESS']

if failed_jobs:
    raise Exception(f"‚ùå Jobs √©chou√©s: {', '.join(failed_jobs)}")

elapsed = time.time() - start_time
print(f"\n‚úÖ Tous les jobs termin√©s avec succ√®s en {elapsed/60:.1f} minutes")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìä √âtape 3 : R√©cup√©rer les outputs

# COMMAND ----------

outputs = {}

for job_name in job_runs.keys():
    dataset = job_name.split('_')[-1]  # site, activite, clymene
    
    table_name = f"abu_catalog.gdp_poc_dev.{dataset}_clymene_all"
    
    # Compter les lignes
    try:
        df = spark.table(table_name)
        row_count = df.count()
        
        outputs[dataset] = {
            'table': table_name,
            'row_count': row_count,
            'columns': df.columns,
            'status': 'SUCCESS'
        }
        
        print(f"‚úÖ {dataset}: {row_count} lignes dans {table_name}")
        
    except Exception as e:
        outputs[dataset] = {
            'table': table_name,
            'status': 'ERROR',
            'error': str(e)
        }
        print(f"‚ùå {dataset}: Erreur - {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üéØ √âtape 4 : Charger la baseline

# COMMAND ----------

# Baseline de r√©f√©rence (√† adapter selon votre structure)
baseline = {
    'site': {
        'expected_row_count': 1000,
        'tolerance_pct': 5,
        'mandatory_columns': ['SITE_CODE', 'SITE_LIBELLE', 'DATE_CREATION'],
        'quality_threshold': 0.95
    },
    'activite': {
        'expected_row_count': 5000,
        'tolerance_pct': 3,
        'mandatory_columns': ['ACTIVITE_ID', 'ACTIVITE_TYPE'],
        'quality_threshold': 0.98
    },
    'clymene': {
        'expected_row_count': 2000,
        'tolerance_pct': 2,
        'mandatory_columns': ['CLYMENE_ID'],
        'quality_threshold': 0.99
    }
}

print("‚úÖ Baseline charg√©e pour 3 datasets")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üîç √âtape 5 : Comparer avec baseline

# COMMAND ----------

comparison_results = {}

for dataset, output in outputs.items():
    if output['status'] != 'SUCCESS':
        comparison_results[dataset] = {
            'status': 'SKIPPED',
            'reason': 'Output non disponible'
        }
        continue
    
    base = baseline[dataset]
    
    # V√©rifier le nombre de lignes
    actual_count = output['row_count']
    expected_count = base['expected_row_count']
    tolerance = int(expected_count * base['tolerance_pct'] / 100)
    
    row_count_ok = abs(actual_count - expected_count) <= tolerance
    
    # V√©rifier les colonnes
    missing_columns = [
        col for col in base['mandatory_columns']
        if col not in output['columns']
    ]
    
    columns_ok = len(missing_columns) == 0
    
    # R√©sultat global
    validation_passed = row_count_ok and columns_ok
    
    comparison_results[dataset] = {
        'status': 'PASSED' if validation_passed else 'FAILED',
        'row_count': {
            'actual': actual_count,
            'expected': expected_count,
            'tolerance': tolerance,
            'passed': row_count_ok
        },
        'columns': {
            'missing': missing_columns,
            'passed': columns_ok
        }
    }
    
    status_icon = "‚úÖ" if validation_passed else "‚ùå"
    print(f"{status_icon} {dataset}: {actual_count} lignes (attendu: {expected_count} ¬± {tolerance})")

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìù √âtape 6 : G√©n√©rer le rapport

# COMMAND ----------

from datetime import datetime
import json

report = {
    'timestamp': datetime.now().isoformat(),
    'baseline_path': BASELINE_PATH,
    'jobs_executed': len(job_runs),
    'jobs_success': sum(1 for r in job_runs.values() if r['status'] == 'SUCCESS'),
    'jobs_failed': sum(1 for r in job_runs.values() if r['status'] == 'FAILED'),
    'datasets_tested': len(comparison_results),
    'datasets_passed': sum(1 for r in comparison_results.values() if r['status'] == 'PASSED'),
    'datasets_failed': sum(1 for r in comparison_results.values() if r['status'] == 'FAILED'),
    'details': comparison_results
}

# Afficher le rapport
print("\n" + "=" * 80)
print("üìä RAPPORT CONSOLID√â")
print("=" * 80)
print(f"\nüïê Date: {report['timestamp']}")
print(f"\n‚úÖ Jobs r√©ussis: {report['jobs_success']}/{report['jobs_executed']}")
print(f"‚úÖ Datasets valid√©s: {report['datasets_passed']}/{report['datasets_tested']}")

if report['datasets_failed'] > 0:
    print(f"\n‚ùå Datasets √©chou√©s: {report['datasets_failed']}")
    for dataset, result in comparison_results.items():
        if result['status'] == 'FAILED':
            print(f"   - {dataset}")

print("\n" + "=" * 80)

# Sauvegarder le rapport
report_json = json.dumps(report, indent=2)
print("\nüìÑ Rapport JSON:")
print(report_json)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ √âtape 7 : R√©sultat final

# COMMAND ----------

if report['datasets_failed'] > 0:
    raise Exception(f"‚ùå Tests √©chou√©s: {report['datasets_failed']} dataset(s) non conforme(s) √† la baseline")

print("‚úÖ TOUS LES TESTS ONT R√âUSSI")
print(f"   {report['jobs_executed']} jobs ex√©cut√©s")
print(f"   {report['datasets_tested']} datasets valid√©s")
print(f"   Conformit√© √† la baseline: 100%")
```

---

## üéØ **R√âPONSES √Ä VOS QUESTIONS**

### **1. Dois-je cr√©er un nouveau job en parall√®le ?**

**OUI**, vous devez cr√©er **2 types de jobs s√©par√©s** :

#### **Type A : Jobs de PRODUCTION** (3 jobs)
```
waxng_pipeline_site
waxng_pipeline_activite
waxng_pipeline_clymene
```
- Utilisables en production
- Un par dataset
- Pas de tests dedans

#### **Type B : Job de TEST** (1 job)
```
waxng_test_orchestrator
```
- Lance les 3 jobs de prod
- Compare avec baseline
- G√©n√®re rapport

---

### **2. Que faire de votre job actuel ?**

**Option 1 : Le supprimer** et utiliser la nouvelle architecture

**Option 2 : Le renommer** en `waxng_pipeline_site` et cr√©er les 2 autres

---

## üìã **PLAN D'ACTION**

```bash
# 1. Cr√©er la nouvelle structure
mkdir -p resources
touch resources/waxng_job.yml
touch resources/waxng_test_orchestrator.yml

# 2. Cr√©er le notebook
mkdir -p notebooks
touch notebooks/test_orchestrator.py

# 3. D√©ployer
databricks bundle deploy --target dev

# 4. Vous aurez maintenant 4 jobs :
#    - waxng_pipeline_site
#    - waxng_pipeline_activite
#    - waxng_pipeline_clymene
#    - waxng_test_orchestrator
```

---

## üéì **AVANTAGES DE CETTE APPROCHE**

‚úÖ **S√©paration prod/test** : Les jobs de prod ne contiennent pas de tests  
‚úÖ **R√©utilisable** : Chaque job peut tourner ind√©pendamment  
‚úÖ **√âvolutif** : Facile d'ajouter de nouveaux datasets  
‚úÖ **Baseline** : Validation contre donn√©es de r√©f√©rence  
‚úÖ **Rapport consolid√©** : Vue d'ensemble sur tous les datasets  

---

**En r√©sum√© : Cr√©ez 4 jobs au lieu d'un seul. Les 3 premiers sont pour la production, le 4√®me est pour orchestrer les tests. C'est exactement ce que votre coll√®gue recommande ! üéØ**
