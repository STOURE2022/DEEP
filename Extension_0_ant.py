# üéØ **ARCHITECTURE DE TESTS RECOMMAND√âE PAR VOTRE COLL√àGUE**

Excellente discussion ! Votre coll√®gue a raison : il propose une **architecture de tests orchestr√©s** plus robuste et √©volutive.

---

## üìä **LES DEUX APPROCHES**

### **Votre vision initiale**
```
Job Test BDD
‚îî‚îÄ‚îÄ Ex√©cute 1 sc√©nario complet
    ‚îî‚îÄ‚îÄ G√©n√®re 1 rapport
```

### **Vision de votre coll√®gue (RECOMMAND√âE)**
```
Job Orchestrateur de Tests
‚îú‚îÄ‚îÄ Lance Job Ingestion 1 (dataset site)
‚îú‚îÄ‚îÄ Lance Job Ingestion 2 (dataset activite)  
‚îú‚îÄ‚îÄ Lance Job Ingestion 3 (dataset clymene)
‚îú‚îÄ‚îÄ R√©cup√®re tous les outputs
‚îú‚îÄ‚îÄ Compare avec baseline (r√©f√©rence)
‚îî‚îÄ‚îÄ G√©n√®re rapport global consolid√©
```

---

## ‚úÖ **IMPL√âMENTATION AVEC GHERKIN**

### **üé≠ Sc√©nario 1 : Test orchestrateur avec baseline**

```gherkin
# features/orchestrator.feature

Feature: Orchestrateur de tests E2E
  En tant que testeur
  Je veux orchestrer plusieurs jobs d'ingestion
  Pour valider le pipeline complet avec une baseline

  Background:
    Given je suis sur Databricks
    And j'ai une baseline de r√©f√©rence

  @orchestrator @e2e
  Scenario: Test E2E complet - 3 datasets
    # √âtape 1 : Lancer les jobs d'ingestion
    When je lance le job "waxng_ingestion_site"
    And je lance le job "waxng_ingestion_activite"
    And je lance le job "waxng_ingestion_clymene"
    
    # √âtape 2 : Attendre la fin
    Then tous les jobs doivent se terminer avec succ√®s
    
    # √âtape 3 : R√©cup√©rer les outputs
    When je r√©cup√®re les outputs de tous les jobs
    
    # √âtape 4 : Comparer avec baseline
    Then je compare les r√©sultats avec la baseline
    And le nombre de lignes doit correspondre √† la baseline
    And les colonnes doivent correspondre √† la baseline
    And la qualit√© des donn√©es doit √™tre >= √† la baseline
    
    # √âtape 5 : Rapport consolid√©
    Then je g√©n√®re un rapport consolid√©
    And le rapport doit contenir tous les jobs
    And le rapport doit afficher les √©carts avec la baseline

  @orchestrator @smoke
  Scenario: Test orchestrateur - Job unique
    When je lance le job "waxng_ingestion_site"
    Then le job doit se terminer avec succ√®s
    When je compare avec la baseline pour "site"
    Then les r√©sultats doivent correspondre √† 95%
```

---

### **üìã Sc√©nario 2 : Validation granulaire par dataset**

```gherkin
# features/baseline_validation.feature

Feature: Validation des r√©sultats contre baseline
  Comparer les outputs de chaque pipeline avec les donn√©es attendues

  Background:
    Given je suis sur Databricks
    And la baseline est charg√©e depuis "s3://baseline/wax/"

  @baseline @site
  Scenario: Valider le dataset SITE contre baseline
    Given le job "waxng_ingestion_site" a √©t√© ex√©cut√©
    When je charge les r√©sultats dans "abu_catalog.gdp_poc_dev.site_clymene_all"
    And je charge la baseline dans "abu_catalog.gdp_poc_dev.baseline_site"
    Then le nombre de lignes doit √™tre identique
    And les colonnes suivantes doivent correspondre:
      | colonne           | tol√©rance |
      | SITE_CODE         | 100%      |
      | SITE_LIBELLE      | 100%      |
      | DATE_CREATION     | 95%       |
      | MONTANT_TOTAL     | 98%       |
    And je g√©n√®re un rapport de comparaison

  @baseline @activite
  Scenario: Valider le dataset ACTIVITE contre baseline
    Given le job "waxng_ingestion_activite" a √©t√© ex√©cut√©
    When je charge les r√©sultats dans "abu_catalog.gdp_poc_dev.activite_all"
    And je charge la baseline dans "abu_catalog.gdp_poc_dev.baseline_activite"
    Then le nombre de lignes doit √™tre dans la tol√©rance de 5%
    And les valeurs NULL doivent √™tre <= 2%
    And je g√©n√®re un rapport de comparaison
```

---

## üîß **IMPL√âMENTATION DES STEPS**

### **`features/steps/orchestrator_steps.py`**

```python
# features/steps/orchestrator_steps.py

"""
Steps pour orchestrer des jobs Databricks et comparer avec baseline
"""

from behave import given, when, then
import time
from datetime import datetime


# ==========================================
# GIVEN - Configuration
# ==========================================

@given('j\'ai une baseline de r√©f√©rence')
def step_load_baseline(context):
    """Charger la baseline de r√©f√©rence"""
    context.baseline = {
        'site': {
            'table': 'abu_catalog.gdp_poc_dev.baseline_site',
            'row_count': 1000,
            'quality_threshold': 0.95
        },
        'activite': {
            'table': 'abu_catalog.gdp_poc_dev.baseline_activite',
            'row_count': 5000,
            'quality_threshold': 0.98
        }
    }
    print("‚úÖ Baseline charg√©e")


@given('la baseline est charg√©e depuis "{path}"')
def step_load_baseline_from_path(context, path):
    """Charger baseline depuis un chemin"""
    context.baseline_path = path
    print(f"‚úÖ Baseline: {path}")


@given('le job "{job_name}" a √©t√© ex√©cut√©')
def step_job_executed(context, job_name):
    """Marquer un job comme ex√©cut√©"""
    if not hasattr(context, 'executed_jobs'):
        context.executed_jobs = []
    context.executed_jobs.append(job_name)
    print(f"‚úÖ Job marqu√© comme ex√©cut√©: {job_name}")


# ==========================================
# WHEN - Actions d'orchestration
# ==========================================

@when('je lance le job "{job_name}"')
def step_launch_job(context, job_name):
    """Lancer un job Databricks"""
    if context.dbutils is None:
        print(f"‚ö†Ô∏è  Mode simulation - job: {job_name}")
        context.job_runs = context.job_runs if hasattr(context, 'job_runs') else {}
        context.job_runs[job_name] = {
            'run_id': 12345,
            'state': 'RUNNING',
            'start_time': datetime.now()
        }
        return
    
    # Code r√©el pour Databricks
    from databricks.sdk import WorkspaceClient
    from databricks.sdk.service import jobs
    
    w = WorkspaceClient()
    
    # Trouver le job par nom
    job_list = w.jobs.list(name=job_name)
    job = next((j for j in job_list), None)
    
    if job is None:
        raise AssertionError(f"Job non trouv√©: {job_name}")
    
    # Lancer le job
    run = w.jobs.run_now(job_id=job.job_id)
    
    # Stocker les infos
    if not hasattr(context, 'job_runs'):
        context.job_runs = {}
    
    context.job_runs[job_name] = {
        'run_id': run.run_id,
        'job_id': job.job_id,
        'state': 'RUNNING'
    }
    
    print(f"üöÄ Job lanc√©: {job_name} (run_id: {run.run_id})")


@when('je r√©cup√®re les outputs de tous les jobs')
def step_collect_outputs(context):
    """R√©cup√©rer les outputs de tous les jobs"""
    context.outputs = {}
    
    for job_name, run_info in context.job_runs.items():
        # Simuler la r√©cup√©ration des outputs
        context.outputs[job_name] = {
            'table': f"abu_catalog.gdp_poc_dev.{job_name.split('_')[-1]}_all",
            'row_count': None,  # √Ä remplir
            'status': 'SUCCESS'
        }
    
    print(f"‚úÖ Outputs r√©cup√©r√©s pour {len(context.outputs)} job(s)")


@when('je compare les r√©sultats avec la baseline')
def step_compare_with_baseline(context):
    """Comparer r√©sultats avec baseline"""
    context.comparison_results = {}
    
    for job_name, output in context.outputs.items():
        dataset_name = job_name.split('_')[-1]
        
        if dataset_name not in context.baseline:
            print(f"‚ö†Ô∏è  Pas de baseline pour: {dataset_name}")
            continue
        
        baseline = context.baseline[dataset_name]
        
        # Comparer le nombre de lignes (simulation)
        actual_count = 1000  # √Ä remplacer par vraie query
        expected_count = baseline['row_count']
        
        context.comparison_results[dataset_name] = {
            'row_count_match': actual_count == expected_count,
            'row_count_actual': actual_count,
            'row_count_expected': expected_count,
            'quality_score': 0.96  # √Ä calculer
        }
    
    print(f"‚úÖ Comparaison effectu√©e pour {len(context.comparison_results)} dataset(s)")


@when('je compare avec la baseline pour "{dataset}"')
def step_compare_single_dataset(context, dataset):
    """Comparer un seul dataset"""
    context.comparison = {
        'dataset': dataset,
        'match_percentage': 0.96  # Simulation
    }
    print(f"‚úÖ Comparaison: {dataset} - 96% de correspondance")


# ==========================================
# THEN - V√©rifications
# ==========================================

@then('tous les jobs doivent se terminer avec succ√®s')
def step_verify_all_jobs_success(context):
    """Attendre et v√©rifier que tous les jobs r√©ussissent"""
    if context.dbutils is None:
        print("‚ö†Ô∏è  Mode simulation - tous les jobs OK")
        return
    
    from databricks.sdk import WorkspaceClient
    w = WorkspaceClient()
    
    max_wait = 3600  # 1 heure max
    start_time = time.time()
    
    while time.time() - start_time < max_wait:
        all_done = True
        
        for job_name, run_info in context.job_runs.items():
            run = w.jobs.get_run(run_id=run_info['run_id'])
            
            if run.state.life_cycle_state in ['PENDING', 'RUNNING']:
                all_done = False
            elif run.state.result_state != 'SUCCESS':
                raise AssertionError(
                    f"Job √©chou√©: {job_name} - {run.state.result_state}"
                )
        
        if all_done:
            print("‚úÖ Tous les jobs termin√©s avec succ√®s")
            return
        
        time.sleep(30)  # Attendre 30s
    
    raise AssertionError("Timeout: jobs non termin√©s apr√®s 1h")


@then('le nombre de lignes doit correspondre √† la baseline')
def step_verify_row_count(context):
    """V√©rifier le nombre de lignes"""
    for dataset, result in context.comparison_results.items():
        if not result['row_count_match']:
            raise AssertionError(
                f"{dataset}: {result['row_count_actual']} lignes "
                f"vs {result['row_count_expected']} attendues"
            )
    
    print("‚úÖ Nombre de lignes OK pour tous les datasets")


@then('les r√©sultats doivent correspondre √† {percentage:d}%')
def step_verify_match_percentage(context, percentage):
    """V√©rifier le pourcentage de correspondance"""
    actual = context.comparison['match_percentage'] * 100
    
    assert actual >= percentage, \
        f"Correspondance insuffisante: {actual}% < {percentage}%"
    
    print(f"‚úÖ Correspondance: {actual}% >= {percentage}%")


@then('je g√©n√®re un rapport consolid√©')
def step_generate_report(context):
    """G√©n√©rer un rapport consolid√©"""
    report = {
        'timestamp': datetime.now().isoformat(),
        'jobs_executed': len(context.job_runs),
        'datasets_compared': len(context.comparison_results),
        'overall_success': True
    }
    
    context.report = report
    print(f"‚úÖ Rapport g√©n√©r√©: {len(context.job_runs)} jobs, "
          f"{len(context.comparison_results)} comparaisons")
```

---

## üèóÔ∏è **ARCHITECTURE COMPL√àTE**

```yaml
# resources/waxng_job.yml

resources:
  jobs:
    # Job 1 : Orchestrateur de tests
    waxng_test_orchestrator:
      name: waxng_test_orchestrator
      
      tasks:
        - task_key: run-tests
          notebook_task:
            notebook_path: /Workspace/Repos/.../notebooks/run_orchestrator_tests
          libraries:
            - whl: ../dist/waxng-1.0.0-py3-none-any.whl
            - pypi:
                package: behave>=1.2.6
    
    # Job 2 : Ingestion SITE (utilisable en prod)
    waxng_ingestion_site:
      name: waxng_ingestion_site
      tasks:
        - task_key: ingest-site
          python_wheel_task:
            package_name: waxng
            entry_point: main
            parameters: ["--dataset=site"]
    
    # Job 3 : Ingestion ACTIVITE (utilisable en prod)
    waxng_ingestion_activite:
      name: waxng_ingestion_activite
      tasks:
        - task_key: ingest-activite
          python_wheel_task:
            package_name: waxng
            entry_point: main
            parameters: ["--dataset=activite"]
```

---

## üéØ **R√âSUM√â**

Votre coll√®gue a raison ! Son approche est meilleure car :

‚úÖ **R√©utilisable** : Les jobs d'ingestion servent en test ET en prod  
‚úÖ **√âvolutif** : Facile d'ajouter de nouveaux datasets  
‚úÖ **Robuste** : Validation avec baseline de r√©f√©rence  
‚úÖ **Consolid√©** : Un seul rapport pour tous les tests  

**L'approche Gherkin/Behave s'adapte parfaitement √† cette architecture !** üöÄ
