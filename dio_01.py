# ‚úÖ **OUI - STRUCTURE RECOMMAND√âE AVEC FICHIERS S√âPAR√âS**

C'est une **excellente pratique** de s√©parer les jobs de production et les jobs de test !

---

## üìÅ **STRUCTURE RECOMMAND√âE**

```
dio/
‚îú‚îÄ‚îÄ resources/
‚îÇ   ‚îú‚îÄ‚îÄ dio_job.yml                    # ‚úÖ Jobs de PRODUCTION
‚îÇ   ‚îú‚îÄ‚îÄ dio_test_job.yml               # ‚úÖ Jobs de TEST
‚îÇ   ‚îî‚îÄ‚îÄ dio_orchestrator_job.yml       # ‚úÖ Job ORCHESTRATEUR (optionnel)
‚îÇ
‚îú‚îÄ‚îÄ databricks.yml                     # Configuration principale
‚îî‚îÄ‚îÄ ...
```

---

## üéØ **POURQUOI S√âPARER ?**

| Crit√®re | Fichier unique | Fichiers s√©par√©s |
|---------|----------------|------------------|
| **Clart√©** | ‚ùå Tout m√©lang√© | ‚úÖ Organisation claire |
| **D√©ploiement** | ‚ùå Tout ou rien | ‚úÖ S√©lectif possible |
| **Maintenance** | ‚ùå Difficile | ‚úÖ Facile |
| **Environnements** | ‚ùå Complexe | ‚úÖ Simple (dev/prod) |
| **Collaboration** | ‚ùå Conflits Git | ‚úÖ Moins de conflits |

---

## üìù **1. `resources/dio_job.yml` (PRODUCTION)**

```yaml
# resources/dio_job.yml
# Jobs de PRODUCTION - Pipelines d'ingestion

resources:
  jobs:
    # ==========================================
    # JOB 1 : PIPELINE SITE (Production)
    # ==========================================
    dio_pipeline_site:
      name: dio_pipeline_site
      description: "Pipeline d'ingestion pour le dataset SITE"
      
      tags:
        environment: production
        team: data-engineering
        project: dio
      
      parameters:
        - name: dataset
          default: "site"
        - name: excel_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/config/config.xlsx"
        - name: zip_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/"
        - name: extract_dir
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/preprocessed/"
      
      tasks:
        # Task 1 : Unzip
        - task_key: unzip
          description: "Extraction ZIP - SITE"
          python_wheel_task:
            package_name: dio
            entry_point: unzip_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
              - "--zip_path={{job.parameters.zip_path}}"
              - "--extract_dir={{job.parameters.extract_dir}}"
          
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
            spark_conf:
              spark.databricks.delta.preview.enabled: "true"
          
          libraries:
            - whl: ../dist/dio-1.0.0-py3-none-any.whl
        
        # Task 2 : Auto Loader
        - task_key: auto-loader
          depends_on:
            - task_key: unzip
          description: "Auto Loader - SITE"
          python_wheel_task:
            package_name: dio
            entry_point: autoloader_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
              - "--source_path={{job.parameters.extract_dir}}"
          
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
        
        # Task 3 : Ingestion finale
        - task_key: dio-ingestion
          depends_on:
            - task_key: auto-loader
          description: "Ingestion finale - SITE"
          python_wheel_task:
            package_name: dio
            entry_point: main
            parameters:
              - "--dataset={{job.parameters.dataset}}"
              - "--excel_path={{job.parameters.excel_path}}"
          
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
      
      # Configuration du job
      max_concurrent_runs: 1
      timeout_seconds: 3600
      
      # Notifications (optionnel)
      email_notifications:
        on_failure:
          - your-email@company.com
    
    # ==========================================
    # JOB 2 : PIPELINE ACTIVITE (Production)
    # ==========================================
    dio_pipeline_activite:
      name: dio_pipeline_activite
      description: "Pipeline d'ingestion pour le dataset ACTIVITE"
      
      tags:
        environment: production
        team: data-engineering
        project: dio
      
      parameters:
        - name: dataset
          default: "activite"
        - name: excel_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/config/config.xlsx"
        - name: zip_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/"
        - name: extract_dir
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/preprocessed/"
      
      tasks:
        - task_key: unzip
          description: "Extraction ZIP - ACTIVITE"
          python_wheel_task:
            package_name: dio
            entry_point: unzip_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          libraries:
            - whl: ../dist/dio-1.0.0-py3-none-any.whl
        
        - task_key: auto-loader
          depends_on:
            - task_key: unzip
          description: "Auto Loader - ACTIVITE"
          python_wheel_task:
            package_name: dio
            entry_point: autoloader_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
        
        - task_key: dio-ingestion
          depends_on:
            - task_key: auto-loader
          description: "Ingestion finale - ACTIVITE"
          python_wheel_task:
            package_name: dio
            entry_point: main
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
      
      max_concurrent_runs: 1
      timeout_seconds: 3600
    
    # ==========================================
    # JOB 3 : PIPELINE CLYMENE (Production)
    # ==========================================
    dio_pipeline_clymene:
      name: dio_pipeline_clymene
      description: "Pipeline d'ingestion pour le dataset CLYMENE"
      
      tags:
        environment: production
        team: data-engineering
        project: dio
      
      parameters:
        - name: dataset
          default: "clymene"
        - name: excel_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/config/config.xlsx"
        - name: zip_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/"
        - name: extract_dir
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/preprocessed/"
      
      tasks:
        - task_key: unzip
          description: "Extraction ZIP - CLYMENE"
          python_wheel_task:
            package_name: dio
            entry_point: unzip_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          libraries:
            - whl: ../dist/dio-1.0.0-py3-none-any.whl
        
        - task_key: auto-loader
          depends_on:
            - task_key: unzip
          description: "Auto Loader - CLYMENE"
          python_wheel_task:
            package_name: dio
            entry_point: autoloader_module
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
        
        - task_key: dio-ingestion
          depends_on:
            - task_key: auto-loader
          description: "Ingestion finale - CLYMENE"
          python_wheel_task:
            package_name: dio
            entry_point: main
            parameters:
              - "--dataset={{job.parameters.dataset}}"
          existing_cluster_id: "{{tasks.unzip.cluster_id}}"
      
      max_concurrent_runs: 1
      timeout_seconds: 3600
```

---

## üìù **2. `resources/dio_test_job.yml` (TESTS)**

```yaml
# resources/dio_test_job.yml
# Jobs de TEST - Tests d'int√©gration BDD

resources:
  jobs:
    # ==========================================
    # JOB DE TEST : TESTS D'INT√âGRATION
    # ==========================================
    dio_integration_tests:
      name: dio_integration_tests
      description: "Tests d'int√©gration BDD pour valider le pipeline DIO"
      
      tags:
        environment: test
        team: data-engineering
        project: dio
        type: integration-tests
      
      parameters:
        - name: test_tags
          default: "integration"
          description: "Tags Behave √† ex√©cuter (ex: integration, @F12_01, @IT_RL1)"
        - name: test_path
          default: "features/integration"
          description: "Chemin vers les features √† tester"
        - name: report_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/test_reports/"
          description: "Chemin pour sauvegarder les rapports"
      
      tasks:
        # Task unique : Ex√©cuter tous les tests
        - task_key: run-integration-tests
          description: "Ex√©cution des tests d'int√©gration avec Behave"
          
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/dio/notebooks/run_integration_tests
            base_parameters:
              test_tags: "{{job.parameters.test_tags}}"
              test_path: "{{job.parameters.test_path}}"
              report_path: "{{job.parameters.report_path}}"
          
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
            spark_conf:
              spark.databricks.delta.preview.enabled: "true"
          
          libraries:
            - whl: ../dist/dio-1.0.0-py3-none-any.whl
            - pypi:
                package: behave>=1.2.6
            - pypi:
                package: databricks-sdk
      
      # Les tests peuvent tourner en parall√®le
      max_concurrent_runs: 3
      timeout_seconds: 7200  # 2 heures max pour les tests
      
      # Notifications
      email_notifications:
        on_failure:
          - data-engineering-team@company.com
    
    # ==========================================
    # JOB DE TEST : TESTS SP√âCIFIQUES PAR TAG
    # ==========================================
    dio_test_format_files:
      name: dio_test_format_files
      description: "Tests sur les formats de fichiers (F12)"
      
      tags:
        environment: test
        team: data-engineering
        project: dio
        test-suite: file-formats
      
      tasks:
        - task_key: test-file-formats
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/dio/notebooks/run_integration_tests
            base_parameters:
              test_tags: "@F12_01_FORMAT_1,@IT_CSV_FORMAT_1,@IT_NONSTRUCT_1"
              test_path: "features/integration/file_formats.feature"
          
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          
          libraries:
            - whl: ../dist/dio-1.0.0-py3-none-any.whl
            - pypi:
                package: behave>=1.2.6
      
      max_concurrent_runs: 1
      timeout_seconds: 3600
    
    dio_test_rejection_rules:
      name: dio_test_rejection_rules
      description: "Tests sur les r√®gles de rejet (RLT)"
      
      tags:
        environment: test
        team: data-engineering
        project: dio
        test-suite: rejection-rules
      
      tasks:
        - task_key: test-rejection
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/dio/notebooks/run_integration_tests
            base_parameters:
              test_tags: "@IT_RL1,@IT_RLT_2"
              test_path: "features/integration/rejection_rules.feature"
          
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          
          libraries:
            - whl: ../dist/dio-1.0.0-py3-none-any.whl
            - pypi:
                package: behave>=1.2.6
      
      max_concurrent_runs: 1
      timeout_seconds: 3600
```

---

## üìù **3. `resources/dio_orchestrator_job.yml` (ORCHESTRATEUR - OPTIONNEL)**

```yaml
# resources/dio_orchestrator_job.yml
# Job ORCHESTRATEUR - Lance les pipelines et valide avec baseline

resources:
  jobs:
    dio_test_orchestrator:
      name: dio_test_orchestrator
      description: "Orchestrateur E2E - Lance tous les pipelines et valide avec baseline"
      
      tags:
        environment: test
        team: data-engineering
        project: dio
        type: orchestrator
      
      parameters:
        - name: baseline_path
          default: "s3://baseline/dio/"
        - name: generate_report
          default: "true"
      
      tasks:
        # Task 1 : Lancer les pipelines de prod
        - task_key: run-all-pipelines
          description: "Lancer tous les pipelines de production"
          
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/dio/notebooks/orchestrator/launch_pipelines
            base_parameters:
              pipelines: "site,activite,clymene"
          
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          
          libraries:
            - whl: ../dist/dio-1.0.0-py3-none-any.whl
            - pypi:
                package: databricks-sdk
        
        # Task 2 : Validation baseline
        - task_key: validate-baseline
          depends_on:
            - task_key: run-all-pipelines
          description: "Comparer les r√©sultats avec la baseline"
          
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/dio/notebooks/orchestrator/validate_baseline
            base_parameters:
              baseline_path: "{{job.parameters.baseline_path}}"
          
          existing_cluster_id: "{{tasks.run-all-pipelines.cluster_id}}"
        
        # Task 3 : G√©n√©rer rapport
        - task_key: generate-report
          depends_on:
            - task_key: validate-baseline
          description: "G√©n√©rer le rapport consolid√©"
          
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/dio/notebooks/orchestrator/generate_report
            base_parameters:
              report_format: "html"
          
          existing_cluster_id: "{{tasks.run-all-pipelines.cluster_id}}"
      
      max_concurrent_runs: 1
      timeout_seconds: 10800  # 3 heures
      
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"  # Tous les jours √† 2h du matin
        timezone_id: "Europe/Paris"
```

---

## üìù **4. `databricks.yml` (CONFIGURATION PRINCIPALE)**

```yaml
# databricks.yml
# Configuration principale du bundle

bundle:
  name: dio

include:
  - resources/*.yml  # ‚úÖ Inclut TOUS les fichiers .yml dans resources/

# D√©finir les cibles (environnements)
targets:
  # Environnement DEV
  dev:
    mode: development
    default: true
    workspace:
      host: https://adb-xxxxx.azuredatabricks.net
    
    # Override pour dev
    resources:
      jobs:
        dio_pipeline_site:
          name: "[DEV] dio_pipeline_site"
        dio_integration_tests:
          name: "[DEV] dio_integration_tests"
  
  # Environnement PROD
  prod:
    mode: production
    workspace:
      host: https://adb-xxxxx.azuredatabricks.net
      root_path: /Workspace/Production/dio
    
    # En prod, on ne d√©ploie PAS les jobs de test
    resources:
      jobs:
        dio_pipeline_site:
          name: "[PROD] dio_pipeline_site"
        dio_pipeline_activite:
          name: "[PROD] dio_pipeline_activite"
        dio_pipeline_clymene:
          name: "[PROD] dio_pipeline_clymene"
    
    # Exclure les jobs de test en prod
    sync:
      exclude:
        - "resources/dio_test_job.yml"
        - "resources/dio_orchestrator_job.yml"
```

---

## üöÄ **COMMANDES DE D√âPLOIEMENT**

### **D√©ployer TOUT en DEV**

```bash
# D√©ploie les 3 fichiers (production + test + orchestrator)
databricks bundle deploy --target dev
```

**R√©sultat :** 6 jobs cr√©√©s
- ‚úÖ `[DEV] dio_pipeline_site`
- ‚úÖ `[DEV] dio_pipeline_activite`
- ‚úÖ `[DEV] dio_pipeline_clymene`
- ‚úÖ `[DEV] dio_integration_tests`
- ‚úÖ `[DEV] dio_test_format_files`
- ‚úÖ `[DEV] dio_test_orchestrator`

---

### **D√©ployer UNIQUEMENT les tests**

```bash
# Option 1 : D√©ployer un fichier sp√©cifique (n√©cessite config)
databricks bundle deploy --target dev --include resources/dio_test_job.yml

# Option 2 : Lancer uniquement le job de test
databricks jobs run-now --job-name "[DEV] dio_integration_tests"
```

---

### **D√©ployer en PROD (sans les tests)**

```bash
# Gr√¢ce √† la config dans databricks.yml, les tests sont exclus
databricks bundle deploy --target prod
```

**R√©sultat :** 3 jobs cr√©√©s
- ‚úÖ `[PROD] dio_pipeline_site`
- ‚úÖ `[PROD] dio_pipeline_activite`
- ‚úÖ `[PROD] dio_pipeline_clymene`
- ‚ùå Tests EXCLUS automatiquement

---

## üìä **AVANTAGES DE CETTE STRUCTURE**

| Avantage | Description |
|----------|-------------|
| **S√©paration claire** | Production ‚â† Test |
| **D√©ploiement s√©lectif** | D√©ployer prod sans tests |
| **Environnements** | Config diff√©rente dev/prod |
| **Maintenance** | Fichiers plus petits, plus lisibles |
| **Collaboration** | Moins de conflits Git |
| **CI/CD** | Facile d'int√©grer dans pipeline |

---

## üéØ **R√âPONSE DIRECTE**

**OUI, cr√©ez :**
1. ‚úÖ `dio_job.yml` ‚Üí Jobs de PRODUCTION
2. ‚úÖ `dio_test_job.yml` ‚Üí Jobs de TEST
3. ‚úÖ `dio_orchestrator_job.yml` ‚Üí Job ORCHESTRATEUR (optionnel)

**Databricks Bundle va automatiquement :**
- Lire tous les fichiers `.yml` dans `resources/`
- Les fusionner
- Les d√©ployer selon l'environnement (dev/prod)

---

**C'est la meilleure pratique ! √áa vous permet de g√©rer production et tests s√©par√©ment. üéØ**
