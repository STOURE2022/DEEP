# ğŸ”§ **ERREUR DÃ‰TECTÃ‰E : `job_name` n'est pas permis !**

Je vois l'erreur dans votre capture d'Ã©cran :

```
Property job_name is not allowed. yaml-schema: dabs://databricks-asset-bundles.json(513)
```

**ProblÃ¨me Ã  la ligne 42 :** La propriÃ©tÃ© `job_name` n'existe pas dans `run_job_task` pour Databricks Asset Bundles.

---

## âœ… **SOLUTION : Utiliser `job_id`**

Il y a **2 solutions** :

### **Solution 1 : RÃ©fÃ©rence par ID (recommandÃ©e pour l'instant)**

```yaml
# waxng_test_job.yml - LIGNE 38-46

- task_key: run-pipeline
  description: "ExÃ©cuter le pipeline WAX NG"
  
  run_job_task:
    job_id: "{{bundle.resources.jobs.waxng_job.id}}"  # âœ… CORRECTION
    job_parameters:
      dataset: "{{job.parameters.dataset}}"
      excel_path: "{{job.parameters.excel_path}}"
      zip_path: "{{job.parameters.zip_path}}"
```

---

### **Solution 2 : Appel via Databricks SDK (alternative)**

Si la solution 1 ne fonctionne pas, utilisez un **notebook** pour lancer le job :

```yaml
# waxng_test_job.yml - LIGNE 38-50

- task_key: run-pipeline
  description: "ExÃ©cuter le pipeline WAX NG"
  
  notebook_task:
    notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/waxng/notebooks/launch_pipeline
    base_parameters:
      pipeline_job_name: "{{job.parameters.pipeline_job_name}}"
      dataset: "{{job.parameters.dataset}}"
      excel_path: "{{job.parameters.excel_path}}"
      zip_path: "{{job.parameters.zip_path}}"
  
  new_cluster:
    spark_version: "14.3.x-scala2.12"
    node_type_id: "Standard_DS3_v2"
    num_workers: 1
  
  libraries:
    - pypi:
        package: databricks-sdk
```

**CrÃ©er le notebook :** `notebooks/launch_pipeline.py`

```python
# Databricks notebook source

from databricks.sdk import WorkspaceClient
from databricks.sdk.service.jobs import RunNow

# ParamÃ¨tres
pipeline_job_name = dbutils.widgets.get("pipeline_job_name")
dataset = dbutils.widgets.get("dataset")
excel_path = dbutils.widgets.get("excel_path")
zip_path = dbutils.widgets.get("zip_path")

print(f"ğŸš€ Lancement du job: {pipeline_job_name}")
print(f"   Dataset: {dataset}")

# Client Databricks
w = WorkspaceClient()

# Trouver le job par nom
jobs = w.jobs.list(name=pipeline_job_name)
job_id = None

for job in jobs:
    if job.settings.name == pipeline_job_name:
        job_id = job.job_id
        break

if not job_id:
    raise Exception(f"Job '{pipeline_job_name}' non trouvÃ©")

print(f"   Job ID: {job_id}")

# Lancer le job
run = w.jobs.run_now(
    job_id=job_id,
    job_parameters={
        "dataset": dataset,
        "excel_path": excel_path,
        "zip_path": zip_path
    }
)

print(f"   Run ID: {run.run_id}")

# Attendre la fin
print("â³ Attente de la fin du job...")

w.jobs.wait_get_run_job_terminated_or_skipped(run_id=run.run_id)

# VÃ©rifier le statut
run_info = w.jobs.get_run(run_id=run.run_id)

if run_info.state.life_cycle_state == "TERMINATED":
    if run_info.state.result_state == "SUCCESS":
        print("âœ… Job terminÃ© avec succÃ¨s")
    else:
        raise Exception(f"âŒ Job Ã©chouÃ©: {run_info.state.state_message}")
else:
    raise Exception(f"âŒ Job dans un Ã©tat inattendu: {run_info.state.life_cycle_state}")
```

---

## ğŸ“ **FICHIER COMPLET CORRIGÃ‰ : `waxng_test_job.yml`**

```yaml
# resources/waxng_test_job.yml
# Jobs de TEST - Validation avec baseline

resources:
  jobs:
    # ==========================================
    # JOB 1 : VALIDATION AVEC BASELINE
    # ==========================================
    waxng_test_validation:
      name: waxng_test_validation
      description: "Valide les rÃ©sultats du pipeline en comparant avec une baseline de rÃ©fÃ©rence"
      
      tags:
        environment: "{{bundle.target}}"
        project: waxng
        type: validation-test
      
      parameters:
        - name: dataset
          default: "site"
          description: "Dataset Ã  valider (site, activite, clymene)"
        
        - name: pipeline_job_name
          default: "waxng_job"
          description: "Nom du job de pipeline Ã  lancer"
        
        - name: excel_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/config/config.xlsx"
          description: "Chemin Excel de configuration"
        
        - name: zip_path
          default: "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/"
          description: "Chemin des fichiers ZIP de test"
        
        - name: tolerance_pct
          default: "5"
          description: "TolÃ©rance en % pour la comparaison (ex: 5 pour Â±5%)"
      
      tasks:
        # ==========================================
        # TASK 1 : Lancer le pipeline de production
        # ==========================================
        - task_key: run-pipeline
          description: "ExÃ©cuter le pipeline WAX NG"
          
          # âœ… SOLUTION : Utiliser un notebook au lieu de run_job_task
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/waxng/notebooks/launch_pipeline
            base_parameters:
              pipeline_job_name: "{{job.parameters.pipeline_job_name}}"
              dataset: "{{job.parameters.dataset}}"
              excel_path: "{{job.parameters.excel_path}}"
              zip_path: "{{job.parameters.zip_path}}"
          
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          
          libraries:
            - whl: ../dist/*.whl
            - pypi:
                package: databricks-sdk
        
        # ==========================================
        # TASK 2 : Comparer avec baseline
        # ==========================================
        - task_key: compare-with-baseline
          depends_on:
            - task_key: run-pipeline
          description: "Comparer les rÃ©sultats avec la baseline de rÃ©fÃ©rence"
          
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/waxng/notebooks/compare_with_baseline
            base_parameters:
              dataset: "{{job.parameters.dataset}}"
              tolerance_pct: "{{job.parameters.tolerance_pct}}"
          
          existing_cluster_id: "{{tasks.run-pipeline.cluster_id}}"
        
        # ==========================================
        # TASK 3 : GÃ©nÃ©rer le rapport
        # ==========================================
        - task_key: generate-report
          depends_on:
            - task_key: compare-with-baseline
          description: "GÃ©nÃ©rer le rapport de validation"
          
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/waxng/notebooks/generate_test_report
            base_parameters:
              dataset: "{{job.parameters.dataset}}"
          
          existing_cluster_id: "{{tasks.run-pipeline.cluster_id}}"
      
      max_concurrent_runs: 2
      timeout_seconds: 7200
      
      email_notifications:
        on_failure:
          - data-team@company.com
    
    # ==========================================
    # JOB 2 : TESTS BDD BEHAVE (optionnel)
    # ==========================================
    waxng_bdd_tests:
      name: waxng_bdd_tests
      description: "Tests BDD avec Behave pour valider les scÃ©narios d'intÃ©gration"
      
      tags:
        environment: "{{bundle.target}}"
        project: waxng
        type: bdd-tests
      
      parameters:
        - name: test_tags
          default: "integration"
          description: "Tags Behave Ã  exÃ©cuter (@F12_01, @IT_RL1, etc.)"
        
        - name: test_feature
          default: "all"
          description: "Feature spÃ©cifique ou 'all'"
      
      tasks:
        - task_key: run-bdd-tests
          description: "ExÃ©cuter les tests BDD avec Behave"
          
          notebook_task:
            notebook_path: /Workspace/Repos/{{workspace.current_user.userName}}/waxng/notebooks/run_bdd_tests
            base_parameters:
              test_tags: "{{job.parameters.test_tags}}"
              test_feature: "{{job.parameters.test_feature}}"
          
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 1
          
          libraries:
            - whl: ../dist/*.whl
            - pypi:
                package: behave>=1.2.6
      
      max_concurrent_runs: 3
      timeout_seconds: 3600
      
      environments:
        - environment_key: default
          spec:
            client: "1"
            dependencies:
              - ../dist/*.whl
              - behave>=1.2.6
```

---

## ğŸ“‚ **FICHIERS Ã€ CRÃ‰ER**

### **1. `notebooks/launch_pipeline.py`**

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # ğŸš€ Lancer le pipeline WAX NG

# COMMAND ----------

from databricks.sdk import WorkspaceClient
from databricks.sdk.service.jobs import RunNow
import time

# COMMAND ----------

# ParamÃ¨tres
dbutils.widgets.text("pipeline_job_name", "waxng_job")
dbutils.widgets.text("dataset", "site")
dbutils.widgets.text("excel_path", "")
dbutils.widgets.text("zip_path", "")

pipeline_job_name = dbutils.widgets.get("pipeline_job_name")
dataset = dbutils.widgets.get("dataset")
excel_path = dbutils.widgets.get("excel_path")
zip_path = dbutils.widgets.get("zip_path")

# COMMAND ----------

print("=" * 80)
print(f"ğŸš€ LANCEMENT DU PIPELINE")
print("=" * 80)
print(f"Job: {pipeline_job_name}")
print(f"Dataset: {dataset}")
print(f"Excel: {excel_path}")
print(f"ZIP: {zip_path}")
print("=" * 80)

# COMMAND ----------

# Client Databricks
w = WorkspaceClient()

# Trouver le job par nom
print(f"\nğŸ” Recherche du job '{pipeline_job_name}'...")

jobs_list = list(w.jobs.list(name=pipeline_job_name))
job_id = None

for job in jobs_list:
    if job.settings.name == pipeline_job_name:
        job_id = job.job_id
        print(f"âœ… Job trouvÃ© - ID: {job_id}")
        break

if not job_id:
    raise Exception(f"âŒ Job '{pipeline_job_name}' non trouvÃ©")

# COMMAND ----------

# Lancer le job
print(f"\nğŸš€ Lancement du job...")

run = w.jobs.run_now(
    job_id=job_id,
    job_parameters={
        "dataset": dataset,
        "excel_path": excel_path,
        "zip_path": zip_path
    }
)

run_id = run.run_id
print(f"âœ… Run lancÃ© - Run ID: {run_id}")
print(f"ğŸ”— URL: https://adb-4320079867405973.13.azuredatabricks.net/#job/{job_id}/run/{run_id}")

# COMMAND ----------

# Attendre la fin
print(f"\nâ³ Attente de la fin du job...")

try:
    # Attendre avec timeout
    w.jobs.wait_get_run_job_terminated_or_skipped(
        run_id=run_id,
        timeout=timedelta(hours=1)
    )
    
    # RÃ©cupÃ©rer les dÃ©tails
    run_info = w.jobs.get_run(run_id=run_id)
    
    print("\n" + "=" * 80)
    if run_info.state.result_state == "SUCCESS":
        print("âœ… JOB TERMINÃ‰ AVEC SUCCÃˆS")
        print("=" * 80)
    else:
        print(f"âŒ JOB Ã‰CHOUÃ‰")
        print(f"Ã‰tat: {run_info.state.result_state}")
        print(f"Message: {run_info.state.state_message}")
        print("=" * 80)
        raise Exception(f"Job Ã©chouÃ©: {run_info.state.state_message}")

except Exception as e:
    print(f"\nâŒ ERREUR: {e}")
    raise
```

---

## ğŸš€ **DÃ‰PLOIEMENT**

```bash
# 1. CrÃ©er le notebook
mkdir -p notebooks
# Copier le contenu de launch_pipeline.py

# 2. Builder
rm -rf dist/ build/ *.egg-info
uv build --wheel

# 3. DÃ©ployer
databricks bundle deploy --target dev

# 4. Tester
databricks jobs run-now --job-name waxng_test_validation \
  --parameters dataset=site
```

---

## âœ… **RÃ‰SUMÃ‰ DE LA CORRECTION**

| Avant (âŒ Erreur) | AprÃ¨s (âœ… CorrigÃ©) |
|-------------------|---------------------|
| `run_job_task: job_name: ...` | `notebook_task: launch_pipeline.py` |
| PropriÃ©tÃ© non supportÃ©e | Appel via SDK Databricks |

---

**VoilÃ  ! L'erreur est corrigÃ©e. Le job de test lance maintenant le pipeline via un notebook avec le SDK Databricks. ğŸ¯**
