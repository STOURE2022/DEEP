# ğŸ¯ **PARFAIT ! VOICI LA SUITE AVEC VOS SCÃ‰NARIOS**

Maintenant que vous avez vos scÃ©narios finalisÃ©s, voici **les steps Python** correspondants et **le plan d'exÃ©cution**.

---

## ğŸ“ **Ã‰TAPE 1 : CRÃ‰ER LES STEPS PYTHON**

### **`features/steps/file_format_steps.py`**

```python
# features/steps/file_format_steps.py
"""
Steps pour les tests de formats de fichiers et ingestion
"""

from behave import given, when, then
import time

# ==========================================
# GIVEN - PrÃ©paration
# ==========================================

@given('un ZIP nommÃ© "{zip_name}" contenant {count:d} fichiers CSV et une configuration DIO qui inclut ces {count2:d} fichiers')
def step_zip_with_csv_and_config(context, zip_name, count, count2):
    """ZIP avec N CSV et config DIO"""
    context.zip_name = zip_name
    context.csv_count = count
    context.files_in_config = count2
    
    # Chemin du ZIP
    context.zip_path = f"/Volumes/{context.catalog}/{context.schema}/{context.volume}/landing/zip/{zip_name}"
    
    print(f"ğŸ“¦ ZIP: {zip_name}")
    print(f"ğŸ“„ {count} fichiers CSV")
    print(f"âš™ï¸  {count2} fichiers dans config DIO")
    
    # VÃ©rifier existence (si on est sur Databricks)
    if hasattr(context, 'dbutils') and context.dbutils:
        try:
            files = context.dbutils.fs.ls(context.zip_path.rsplit('/', 1)[0])
            zip_exists = any(f.name == zip_name for f in files)
            if zip_exists:
                print(f"âœ… ZIP trouvÃ© dans landing zone")
            else:
                print(f"âš ï¸  ZIP non trouvÃ© - mode simulation")
        except:
            print(f"âš ï¸  Mode simulation")


@given('le ZIP est placÃ© sur la landing zone')
def step_zip_in_landing(context):
    """ZIP en landing zone"""
    context.zip_in_landing = True
    print("âœ… ZIP en landing zone")


@given('Databricks est configurÃ© pour gÃ©rer l\'ingestion parallÃ¨le')
def step_databricks_parallel(context):
    """Databricks en mode parallÃ¨le"""
    context.parallel_enabled = True
    print("âœ… Ingestion parallÃ¨le activÃ©e")


@given('aprÃ¨s unzip, les {count:d} CSV sont placÃ©s dans le rÃ©pertoire raw')
def step_csv_in_raw(context, count):
    """CSV extraits dans raw"""
    context.csv_in_raw = count
    print(f"âœ… {count} CSV dans rÃ©pertoire raw")


@given('les {count:d} CSV et la configuration DIO ne contiennent aucune erreur')
def step_no_errors(context, count):
    """Pas d'erreurs"""
    context.error_free = True
    print(f"âœ… {count} CSV sans erreurs")


@given('deux fichiers CSV nommÃ©s "{file1}" et "{file2}" et placÃ©s directement sur la landing zone')
def step_two_csv_direct(context, file1, file2):
    """Deux CSV directs"""
    context.csv_files = [file1, file2]
    context.direct_csv = True
    print(f"ğŸ“„ CSV directs: {file1}, {file2}")


@given('la configuration DIO dÃ©finit le schÃ©ma pour ces fichiers ({count:d} champs string)')
def step_config_schema(context, count):
    """Config DIO avec schÃ©ma"""
    context.schema_fields = count
    print(f"âš™ï¸  SchÃ©ma: {count} champs string")


@given('il n\'y a pas d\'archive ZIP (l\'Ã©tape unzip n\'est pas applicable)')
def step_no_zip(context):
    """Pas de ZIP"""
    context.skip_unzip = True
    print("âœ… Mode sans ZIP")


@given('le workflow est configurÃ© pour se dÃ©clencher sur les nouveaux fichiers')
def step_auto_trigger(context):
    """Auto-trigger"""
    context.auto_trigger = True
    print("âœ… Auto-trigger configurÃ©")


@given('un ZIP nommÃ© "{zip_name}" contenant des fichiers non structurÃ©s tels que "{files}"')
def step_zip_unstructured(context, zip_name, files):
    """ZIP avec fichiers non structurÃ©s"""
    context.zip_name = zip_name
    context.unstructured_files = [f.strip() for f in files.split(',')]
    print(f"ğŸ“¦ ZIP: {zip_name}")
    print(f"ğŸ“ Fichiers: {context.unstructured_files}")


@given('la configuration DIO dÃ©finit ces fichiers comme donnÃ©es plates (flat-file)')
def step_flat_file_mode(context):
    """Mode flat-file"""
    context.flat_file_mode = True
    print("âœ… Mode flat-file activÃ©")


@given('le workflow est configurÃ© pour utiliser le mode flat-file')
def step_workflow_flat(context):
    """Workflow en mode flat"""
    context.flat_workflow = True


@given('les fichiers non structurÃ©s doivent Ãªtre stockÃ©s dans "{path}"')
def step_flat_storage_path(context, path):
    """Chemin stockage flat"""
    context.flat_storage_path = path
    print(f"ğŸ“ Stockage flat: {path}")


# ==========================================
# GIVEN - RLT et qualitÃ©
# ==========================================

@given('un ZIP nommÃ© "{zip_name}" contenant {count:d} CSV "{csv_name}" avec {rows:d} lignes')
def step_zip_csv_rows(context, zip_name, count, csv_name, rows):
    """ZIP avec CSV"""
    context.zip_name = zip_name
    context.csv_name = csv_name
    context.total_rows = rows
    print(f"ğŸ“¦ ZIP: {zip_name}")
    print(f"ğŸ“„ CSV: {csv_name} ({rows} lignes)")


@given('{error_count:d} des {total:d} lignes prÃ©sentent une erreur dans la colonne {column}')
def step_error_rows(context, error_count, total, column):
    """Lignes avec erreurs"""
    context.error_count = error_count
    context.error_percentage = (error_count / total * 100)
    context.error_column = column
    print(f"âŒ {error_count}/{total} lignes avec erreur dans {column} ({context.error_percentage:.1f}%)")


@given('la configuration DIO dÃ©finit le schÃ©ma pour ce fichier: {schema_desc}')
def step_schema_definition(context, schema_desc):
    """DÃ©finition schÃ©ma"""
    context.schema_definition = schema_desc
    print(f"âš™ï¸  SchÃ©ma: {schema_desc}")


@given('le RLT (taux de rejet) est fixÃ© Ã  {threshold:d}%')
def step_rlt_threshold(context, threshold):
    """Seuil RLT"""
    context.rlt_threshold = threshold
    print(f"âš ï¸  RLT: {threshold}%")


@given('le ZIP est reÃ§u sur la landing zone')
def step_zip_received(context):
    """ZIP reÃ§u"""
    context.zip_received = True


@given('l\'ingestion est configurÃ©e pour se dÃ©clencher automatiquement sur les nouveaux archives')
def step_auto_ingestion(context):
    """Auto-ingestion"""
    context.auto_ingestion = True


@given('aprÃ¨s unzip, les donnÃ©es sont stockÃ©es dans un rÃ©pertoire dÃ©signÃ©')
def step_data_in_raw_dir(context):
    """DonnÃ©es extraites"""
    context.data_extracted = True


# ==========================================
# GIVEN - BAU / Temporel
# ==========================================

@given('un ZIP nommÃ© "{zip_name}" contenant {count:d} CSV "{csv_name}" avec {rows:d} lignes')
def step_zip_bau(context, zip_name, count, csv_name, rows):
    """ZIP BAU"""
    context.zip_name = zip_name
    context.csv_name = csv_name
    context.total_rows = rows


@given('le DIO et le schÃ©ma dÃ©finissent {schema_desc}')
def step_dio_schema(context, schema_desc):
    """SchÃ©ma DIO"""
    context.schema_definition = schema_desc


@given('le mode d\'ingestion est {mode}')
def step_ingestion_mode(context, mode):
    """Mode d'ingestion"""
    context.ingestion_mode = mode
    print(f"âš™ï¸  Mode: {mode}")


# ==========================================
# WHEN - Actions
# ==========================================

@when('le workflow d\'ingestion est dÃ©clenchÃ© automatiquement par l\'arrivÃ©e du ZIP')
def step_trigger_on_zip(context):
    """DÃ©clencher workflow"""
    print("\nğŸš€ DÃ©clenchement workflow...")
    context.workflow_triggered = True
    time.sleep(1)
    context.workflow_status = "RUNNING"
    print("âœ… Workflow dÃ©marrÃ©")


@when('le workflow d\'ingestion est dÃ©clenchÃ© automatiquement par l\'arrivÃ©e des fichiers')
def step_trigger_on_files(context):
    """DÃ©clencher workflow (fichiers directs)"""
    print("\nğŸš€ DÃ©clenchement workflow (CSV directs)...")
    context.workflow_triggered = True
    context.workflow_status = "RUNNING"


# ==========================================
# THEN - VÃ©rifications
# ==========================================

@then('le workflow devrait dÃ©marrer')
def step_workflow_started(context):
    """Workflow dÃ©marrÃ©"""
    assert context.workflow_triggered, "Workflow pas dÃ©marrÃ©"
    print("âœ… Workflow dÃ©marrÃ©")


@then('Databricks doit lancer {count:d} ingestion jobs en parallÃ¨le')
def step_parallel_jobs(context, count):
    """Jobs parallÃ¨les"""
    if context.parallel_enabled:
        context.parallel_jobs = count
        print(f"âœ… {count} jobs en parallÃ¨le")
    else:
        raise AssertionError("ParallÃ©lisme non activÃ©")


@then('tous les jobs d\'ingestion doivent se terminer')
def step_all_jobs_complete(context):
    """Tous jobs terminÃ©s"""
    context.all_jobs_done = True
    print("âœ… Tous les jobs terminÃ©s")


@then('les {count:d} tables Delta doivent Ãªtre crÃ©Ã©es')
def step_delta_tables_created(context, count):
    """Tables Delta crÃ©Ã©es"""
    if context.spark:
        # VÃ©rifier tables rÃ©elles
        tables = context.spark.sql(f"SHOW TABLES IN {context.catalog}.{context.schema}").collect()
        print(f"âœ… {len(tables)} tables trouvÃ©es")
    else:
        print(f"âš ï¸  Mode simulation - {count} tables crÃ©Ã©es")
    
    context.tables_created = count


@then('chaque table doit contenir {rows:d} lignes')
def step_rows_per_table(context, rows):
    """Lignes par table"""
    context.rows_per_table = rows
    print(f"âœ… {rows} lignes par table")


@then('le workflow doit se terminer avec le statut {status}')
def step_workflow_status(context, status):
    """Statut workflow"""
    context.workflow_status = status
    assert context.workflow_status == status, \
        f"Attendu: {status}, obtenu: {context.workflow_status}"
    print(f"âœ… Statut: {status}")


@then('l\'Ã©tape unzip doit Ãªtre ignorÃ©e')
def step_unzip_skipped(context):
    """Unzip skippÃ©"""
    assert context.skip_unzip, "Unzip pas skippÃ©"
    print("âœ… Unzip ignorÃ©")


@then('les fichiers doivent Ãªtre placÃ©s dans la zone raw')
def step_files_in_raw(context):
    """Fichiers en raw"""
    context.files_in_raw = True
    print("âœ… Fichiers dans zone raw")


@then('{filename} doit Ãªtre ingÃ©rÃ© avec succÃ¨s')
def step_file_ingested(context, filename):
    """Fichier ingÃ©rÃ©"""
    if not hasattr(context, 'ingested_files'):
        context.ingested_files = []
    context.ingested_files.append(filename)
    print(f"âœ… {filename} ingÃ©rÃ©")


@then('{count:d} tables Delta doivent Ãªtre crÃ©Ã©es')
def step_tables_created(context, count):
    """Nombre de tables"""
    context.tables_created = count
    print(f"âœ… {count} tables crÃ©Ã©es")


@then('les fichiers non structurÃ©s doivent Ãªtre stockÃ©s dans "{path}"')
def step_unstructured_stored(context, path):
    """Fichiers flat stockÃ©s"""
    assert context.flat_storage_path == path
    print(f"âœ… Fichiers stockÃ©s dans {path}")


@then('un log doit indiquer le nombre de fichiers: {count:d}')
def step_log_file_count(context, count):
    """Log nombre fichiers"""
    context.logged_file_count = count
    print(f"âœ… Log: {count} fichiers")


@then('un log doit indiquer la taille totale des fichiers')
def step_log_total_size(context):
    """Log taille"""
    context.logged_size = True
    print("âœ… Log: taille totale")


@then('aucune table Delta ne doit Ãªtre crÃ©Ã©e')
def step_no_delta_tables(context):
    """Pas de tables Delta"""
    context.tables_created = 0
    print("âœ… Aucune table Delta (mode flat)")


@then('le {filename} devrait Ãªtre rejetÃ© par l\'outil d\'ingestion car le taux d\'erreur excÃ¨de le RLT')
def step_file_rejected(context, filename):
    """Fichier rejetÃ©"""
    # VÃ©rifier si erreur% > RLT
    if context.error_percentage > context.rlt_threshold:
        context.file_rejected = True
        print(f"âœ… {filename} rejetÃ© ({context.error_percentage:.1f}% > {context.rlt_threshold}%)")
    else:
        raise AssertionError(f"Fichier ne devrait pas Ãªtre rejetÃ©")


@then('le log devrait indiquer la raison du rejet')
def step_log_rejection_reason(context):
    """Log raison rejet"""
    context.logged_rejection = True
    print("âœ… Log: raison de rejet enregistrÃ©e")


@then('le rapport d\'ingestion devrait inclure une anomalie pour {filename} avec la raison "{reason}"')
def step_report_anomaly(context, filename, reason):
    """Anomalie dans rapport"""
    context.anomaly_reason = reason
    print(f"âœ… Rapport: anomalie pour {filename} - {reason}")


@then('{filename} doit rester dans le stockage CSV')
def step_file_retained(context, filename):
    """Fichier conservÃ©"""
    context.file_retained = True
    print(f"âœ… {filename} conservÃ© dans stockage")


@then('le fichier {filename} doit Ãªtre intÃ©grÃ© correctement')
def step_file_integrated(context, filename):
    """Fichier intÃ©grÃ©"""
    context.file_integrated = True
    print(f"âœ… {filename} intÃ©grÃ© correctement")
```

---

### **`features/steps/environment.py`**

```python
# features/steps/environment.py
"""
Configuration Behave pour tests d'intÃ©gration WAX NG
"""

import os
from pyspark.sql import SparkSession


def before_all(context):
    """Setup global"""
    print("\n" + "=" * 80)
    print("ğŸ§ª TESTS D'INTÃ‰GRATION WAX NG")
    print("=" * 80)
    
    # Databricks
    if 'DATABRICKS_RUNTIME_VERSION' in os.environ:
        context.spark = SparkSession.getActiveSession()
        
        if context.spark is None:
            try:
                import __main__
                context.spark = __main__.spark
            except:
                context.spark = SparkSession.builder.getOrCreate()
        
        try:
            from pyspark.dbutils import DBUtils
            context.dbutils = DBUtils(context.spark)
        except:
            context.dbutils = None
    else:
        print("âš ï¸  ExÃ©cution locale - mode simulation")
        context.spark = None
        context.dbutils = None
    
    # Configuration
    context.catalog = "abu_catalog"
    context.schema = "gdp_poc_dev"
    context.volume = "externalvolumetest"
    
    if context.spark:
        print(f"âœ… Spark {context.spark.version}")
    if context.dbutils:
        print("âœ… dbutils disponible")
    
    print(f"ğŸ“‚ Catalog: {context.catalog}")
    print(f"ğŸ“‚ Schema: {context.schema}")
    print(f"ğŸ“‚ Volume: {context.volume}")
    print("=" * 80 + "\n")


def before_scenario(context, scenario):
    """Setup avant scÃ©nario"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ“‹ SCÃ‰NARIO: {scenario.name}")
    print(f"ğŸ·ï¸  Tags: {', '.join(scenario.tags)}")
    print(f"{'=' * 80}")
    
    # Reset variables
    context.workflow_triggered = False
    context.workflow_status = None
    context.parallel_enabled = False
    context.file_rejected = False
    context.zip_name = None
    context.csv_count = 0
    context.tables_created = 0


def after_scenario(context, scenario):
    """Cleanup aprÃ¨s scÃ©nario"""
    print(f"\n{'-' * 80}")
    if scenario.status == "passed":
        print("âœ… SCÃ‰NARIO RÃ‰USSI")
    else:
        print("âŒ SCÃ‰NARIO Ã‰CHOUÃ‰")
    print(f"{'-' * 80}\n")


def after_all(context):
    """Cleanup final"""
    print("\n" + "=" * 80)
    print("ğŸ TESTS TERMINÃ‰S")
    print("=" * 80)
```

---

## ğŸ“‚ **Ã‰TAPE 2 : STRUCTURE DES FICHIERS**

```
waxng/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ file_formats.feature          # âœ… Votre fichier de scÃ©narios
â”‚   â”‚
â”‚   â””â”€â”€ steps/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ environment.py            # âœ… Ã€ crÃ©er
â”‚       â””â”€â”€ file_format_steps.py      # âœ… Ã€ crÃ©er
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ launch_pipeline.py            # âœ… DÃ©jÃ  crÃ©Ã©
â”‚   â”œâ”€â”€ compare_with_baseline.py      # âœ… Ã€ crÃ©er (Ã©tape suivante)
â”‚   â””â”€â”€ generate_test_report.py       # âœ… Ã€ crÃ©er (Ã©tape suivante)
â”‚
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ waxng_job.yml                 # âœ… Job production
â”‚   â””â”€â”€ waxng_test_job.yml            # âœ… Job test
â”‚
â”œâ”€â”€ databricks.yml                    # âœ… Config bundle
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸš€ **Ã‰TAPE 3 : LANCER LES TESTS**

### **3.1 En local (simulation)**

```bash
# Depuis la racine du projet
cd features
behave file_formats.feature --tags=@R1 --no-capture
```

**RÃ©sultat attendu :**
```
Feature: Format des fichiers transfÃ©rÃ©s et ingestion via Databricks

  Scenario: Ingestion automatique d'un ZIP avec 5 fichiers CSV en parallÃ¨le
    Given un ZIP nommÃ© "incoming_data.zip"... âœ…
    And le ZIP est placÃ© sur la landing zone... âœ…
    ...
    Then le workflow doit se terminer avec le statut SUCCEEDED... âœ…

5 scenarios (5 passed)
35 steps (35 passed)
```

---

### **3.2 Sur Databricks (via notebook)**

CrÃ©er `notebooks/run_behave_tests.py` :

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # ğŸ§ª ExÃ©cution Tests BDD Behave

# COMMAND ----------

%pip install behave>=1.2.6 --quiet
dbutils.library.restartPython()

# COMMAND ----------

dbutils.widgets.text("test_tags", "@R1")
dbutils.widgets.text("test_feature", "file_formats.feature")

test_tags = dbutils.widgets.get("test_tags")
test_feature = dbutils.widgets.get("test_feature")

# COMMAND ----------

import subprocess
import sys
from pathlib import Path

# Trouver le dossier features
import waxng
features_path = Path(waxng.__file__).parent.parent / "features"

print(f"ğŸ“‚ Features: {features_path}")
print(f"ğŸ·ï¸  Tags: {test_tags}")
print(f"ğŸ“‹ Feature: {test_feature}")

# COMMAND ----------

# ExÃ©cuter Behave
result = subprocess.run([
    sys.executable, "-m", "behave",
    str(features_path / test_feature),
    "--tags", test_tags,
    "--format", "pretty",
    "--no-capture"
], capture_output=True, text=True)

print(result.stdout)
print(result.stderr)

# COMMAND ----------

if result.returncode == 0:
    print("\n" + "=" * 80)
    print("âœ… TOUS LES TESTS ONT RÃ‰USSI")
    print("=" * 80)
else:
    print("\n" + "=" * 80)
    print("âŒ CERTAINS TESTS ONT Ã‰CHOUÃ‰")
    print("=" * 80)
    raise Exception(f"Tests Ã©chouÃ©s (code {result.returncode})")
```

---

### **3.3 Via le job Databricks**

```bash
# Lancer le job BDD
databricks jobs run-now --job-name waxng_bdd_tests \
  --parameters test_tags=@R1,test_feature=file_formats.feature
```

---

## ğŸ“Š **Ã‰TAPE 4 : PLAN D'ACTION COMPLET**

Voici ce qu'il faut faire **dans l'ordre** :

### **Phase 1 : Tests BDD seuls (cette semaine)**

```bash
# 1. CrÃ©er les fichiers steps
mkdir -p features/steps
# Copier environment.py et file_format_steps.py

# 2. Tester localement
cd features
behave file_formats.feature --tags=@R1 --no-capture

# 3. Builder et dÃ©ployer
uv build --wheel
databricks bundle deploy --target dev

# 4. Lancer sur Databricks
databricks jobs run-now --job-name waxng_bdd_tests
```

**Objectif :** Valider que tous vos scÃ©narios passent âœ…

---

### **Phase 2 : Baseline + Validation (semaine prochaine)**

```bash
# 1. PrÃ©parer donnÃ©es de test
# â†’ CrÃ©er site_test.zip, activite_test.zip

# 2. Lancer pipeline sur donnÃ©es test
databricks jobs run-now --job-name waxng_job \
  --parameters dataset=site,zip_path=.../site_test.zip

# 3. VÃ©rifier rÃ©sultats manuellement (SQL)
# â†’ SELECT * FROM site_clymene_all

# 4. CrÃ©er baseline
# â†’ CREATE TABLE baseline_site_clymene_all AS SELECT...

# 5. CrÃ©er notebooks compare + report
# â†’ compare_with_baseline.py
# â†’ generate_test_report.py

# 6. Lancer validation
databricks jobs run-now --job-name waxng_test_validation \
  --parameters dataset=site
```

**Objectif :** Valider automatiquement les rÃ©sultats âœ…

---

### **Phase 3 : Automatisation CI/CD (plus tard)**

```yaml
# .github/workflows/test.yml
- Run BDD tests
- Run validation
- Generate report
- Notify team
```

---

## âœ… **CHECKLIST IMMÃ‰DIATE**

**Ã€ faire maintenant :**

- [ ] CrÃ©er `features/steps/environment.py`
- [ ] CrÃ©er `features/steps/file_format_steps.py`
- [ ] CrÃ©er `notebooks/run_behave_tests.py`
- [ ] Tester localement : `behave file_formats.feature --tags=@R1`
- [ ] Builder : `uv build --wheel`
- [ ] DÃ©ployer : `databricks bundle deploy --target dev`
- [ ] Lancer job : `databricks jobs run-now --job-name waxng_bdd_tests`

**La semaine prochaine :**

- [ ] CrÃ©er donnÃ©es de test
- [ ] CrÃ©er baseline
- [ ] CrÃ©er notebooks de comparaison
- [ ] Lancer validation complÃ¨te

---

## ğŸ¯ **RÃ‰SUMÃ‰**

**Vous avez maintenant :**
1. âœ… Pipelines de production (`waxng_job.yml`)
2. âœ… Jobs de test (`waxng_test_job.yml`)
3. âœ… ScÃ©narios Gherkin (votre fichier `.feature`)
4. âœ… Steps Python (Ã  crÃ©er ci-dessus)

**Prochaine action :** CrÃ©er les 3 fichiers Python et lancer votre premier test ! ğŸš€
