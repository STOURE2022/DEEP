# features/steps/environment.py

"""
Configuration de l'environnement Behave pour les tests BDD
Compatible Databricks et ex√©cution locale
"""

from pyspark.sql import SparkSession
import sys
import os


def _get_spark_session():
    """
    Obtenir la session Spark selon l'environnement
    
    Returns:
        SparkSession: Session Spark active
    """
    # ‚úÖ SUR DATABRICKS : TOUJOURS utiliser la session active
    if 'DATABRICKS_RUNTIME_VERSION' in os.environ:
        print("üéØ Environnement: Databricks")
        
        # R√©cup√©rer la session EXISTANTE (NE PAS EN CR√âER UNE NOUVELLE)
        spark = SparkSession.getActiveSession()
        
        if spark is None:
            raise RuntimeError(
                "‚ùå Aucune session Spark active trouv√©e sur Databricks.\n"
                "   Cela ne devrait jamais arriver dans un cluster Databricks."
            )
        
        return spark
    
    # ‚úÖ EX√âCUTION LOCALE : Cr√©er une session
    else:
        print("üéØ Environnement: Local")
        
        return SparkSession.builder \
            .appName("WAX_BDD_Tests_Local") \
            .config("spark.sql.shuffle.partitions", "2") \
            .config("spark.driver.memory", "2g") \
            .master("local[*]") \
            .getOrCreate()


def before_all(context):
    """Setup global AVANT tous les tests"""
    print("\n" + "=" * 80)
    print("üöÄ INITIALISATION ENVIRONNEMENT DE TEST BDD")
    print("=" * 80)
    
    try:
        # Obtenir Spark
        context.spark = _get_spark_session()
        
        if context.spark is None:
            raise RuntimeError("‚ùå Impossible d'obtenir une session Spark")
        
        print(f"‚úÖ Spark {context.spark.version} disponible")
        print(f"üìç Spark Master: {context.spark.sparkContext.master}")
        
        # V√©rifier dbutils (Databricks uniquement)
        try:
            from pyspark.dbutils import DBUtils
            context.dbutils = DBUtils(context.spark)
            print("‚úÖ dbutils disponible")
        except Exception as e:
            context.dbutils = None
            print("‚ö†Ô∏è  dbutils non disponible (mode local)")
            
    except Exception as e:
        print(f"‚ùå Erreur initialisation Spark: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Configuration
    context.catalog = "abu_catalog"
    context.schema = "gdp_poc_dev"
    context.volume = "externalvolumetest"
    
    print(f"üìÇ Catalog: {context.catalog}")
    print(f"üìÇ Schema: {context.schema}")
    print("=" * 80 + "\n")


def after_all(context):
    """Cleanup APR√àS tous les tests"""
    print("\n" + "=" * 80)
    print("üßπ NETTOYAGE ENVIRONNEMENT DE TEST")
    print("=" * 80)
    
    # ‚úÖ NE JAMAIS arr√™ter Spark sur Databricks
    if 'DATABRICKS_RUNTIME_VERSION' not in os.environ:
        if hasattr(context, 'spark') and context.spark is not None:
            try:
                context.spark.stop()
                print("‚úÖ Spark arr√™t√© proprement (mode local)")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur arr√™t Spark: {e}")
    else:
        print("‚ÑπÔ∏è  Session Spark conserv√©e (Databricks)")
    
    print("=" * 80 + "\n")


def before_scenario(context, scenario):
    """Setup AVANT chaque sc√©nario"""
    print("\n" + "-" * 80)
    print(f"üìã SC√âNARIO: {scenario.name}")
    print(f"üè∑Ô∏è  Tags: {', '.join(scenario.tags) if scenario.tags else 'Aucun'}")
    print("-" * 80)
    
    # R√©initialiser les variables de contexte
    context.zip_path = None
    context.extract_path = None
    context.zip_exists = False
    context.unzip_success = False
    context.unzip_error = None
    context.error_message = None
    context.error_type = None


def after_scenario(context, scenario):
    """Cleanup APR√àS chaque sc√©nario"""
    if scenario.status == "failed":
        print(f"\n‚ùå SC√âNARIO √âCHOU√â: {scenario.name}")
        
        # Afficher des infos de debug
        if hasattr(context, 'unzip_error') and context.unzip_error:
            print(f"\nüîç D√âTAILS DE L'ERREUR:")
            print(f"   Type: {type(context.unzip_error).__name__}")
            print(f"   Message: {context.unzip_error}")
    else:
        print(f"\n‚úÖ SC√âNARIO R√âUSSI: {scenario.name}")
    
    print("-" * 80)


def before_feature(context, feature):
    """Setup AVANT chaque feature"""
    print("\n" + "=" * 80)
    print(f"üéØ FEATURE: {feature.name}")
    print("=" * 80)


def after_feature(context, feature):
    """Cleanup APR√àS chaque feature"""
    passed = sum(1 for s in feature.scenarios if s.status == "passed")
    failed = sum(1 for s in feature.scenarios if s.status == "failed")
    skipped = sum(1 for s in feature.scenarios if s.status == "skipped")
    total = len(feature.scenarios)
    
    print("\n" + "=" * 80)
    print(f"üìä R√âSULTATS FEATURE: {feature.name}")
    print(f"   ‚úÖ R√©ussis: {passed}/{total}")
    
    if failed > 0:
        print(f"   ‚ùå √âchou√©s: {failed}/{total}")
    
    if skipped > 0:
        print(f"   ‚è≠Ô∏è  Ignor√©s: {skipped}/{total}")
    
    print("=" * 80)
