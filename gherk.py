# ğŸ§ª **PREMIER TEST BDD AVEC GHERKIN - MODULE UNZIP**

## ğŸ“ **STRUCTURE COMPLÃˆTE**

```
waxng/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ unzip.feature              # âœ… ScÃ©narios Gherkin
â”‚   â””â”€â”€ steps/
â”‚       â”œâ”€â”€ __init__.py            # âœ… Fichier vide
â”‚       â”œâ”€â”€ environment.py         # âœ… Configuration Behave
â”‚       â””â”€â”€ unzip_steps.py         # âœ… ImplÃ©mentation des steps
â”œâ”€â”€ waxng/
â”‚   â”œâ”€â”€ unzip_module.py            # Votre module existant
â”‚   â””â”€â”€ run_bdd_tests.py           # âœ… Script d'exÃ©cution
â””â”€â”€ setup.py                        # Ã€ modifier
```

---

## ğŸ“ **1. FICHIER : `features/unzip.feature`**

```gherkin
# features/unzip.feature

Feature: DÃ©compression de fichiers ZIP
  En tant que data engineer
  Je veux dÃ©compresser des fichiers ZIP automatiquement
  Pour prÃ©parer les donnÃ©es avant ingestion

  Background:
    Given Spark est disponible
    And les chemins de base sont configurÃ©s

  @smoke @unzip
  Scenario: DÃ©compresser un fichier ZIP valide
    Given un fichier ZIP existe Ã  "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/test_valid.zip"
    When je dÃ©compresse le fichier vers "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/preprocessed/"
    Then la dÃ©compression doit rÃ©ussir
    And les fichiers extraits doivent exister dans le dossier de destination

  @unzip
  Scenario: Compter les fichiers extraits
    Given un fichier ZIP existe Ã  "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/site_20251112_173355.zip"
    When je dÃ©compresse le fichier vers "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/preprocessed/"
    Then au moins 1 fichier doit Ãªtre extrait

  @unzip @negative
  Scenario: GÃ©rer un fichier ZIP inexistant
    Given un fichier ZIP n'existe pas Ã  "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/nonexistent.zip"
    When je tente de dÃ©compresser le fichier
    Then une erreur doit Ãªtre levÃ©e
    And le message d'erreur doit contenir "not found" ou "does not exist"

  @unzip @negative
  Scenario: GÃ©rer un fichier ZIP corrompu
    Given un fichier ZIP corrompu existe Ã  "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest/landing/zip/corrupt.zip"
    When je tente de dÃ©compresser le fichier vers "/tmp/test_extract/"
    Then une erreur doit Ãªtre levÃ©e
    And le message d'erreur doit contenir "corrupt" ou "invalid"
```

---

## ğŸ **2. FICHIER : `features/steps/unzip_steps.py`**

```python
# features/steps/unzip_steps.py

"""
Step definitions pour les tests de dÃ©compression ZIP
"""

from behave import given, when, then
from pyspark.sql import SparkSession
import traceback


# ==========================================
# GIVEN STEPS
# ==========================================

@given('Spark est disponible')
def step_spark_available(context):
    """VÃ©rifier que Spark est disponible"""
    if not hasattr(context, 'spark') or context.spark is None:
        raise AssertionError("âŒ Spark n'est pas disponible dans le contexte")
    
    print(f"âœ… Spark {context.spark.version} disponible")


@given('les chemins de base sont configurÃ©s')
def step_paths_configured(context):
    """Configurer les chemins de base"""
    context.base_path = "/Volumes/abu_catalog/gdp_poc_dev/externalvolumetest"
    context.landing_zip = f"{context.base_path}/landing/zip"
    context.preprocessed = f"{context.base_path}/preprocessed"
    
    print(f"ğŸ“‚ Landing ZIP: {context.landing_zip}")
    print(f"ğŸ“‚ Preprocessed: {context.preprocessed}")


@given('un fichier ZIP existe Ã  "{zip_path}"')
def step_zip_exists(context, zip_path):
    """VÃ©rifier qu'un fichier ZIP existe"""
    context.zip_path = zip_path
    
    try:
        from pyspark.dbutils import DBUtils
        dbutils = DBUtils(context.spark)
        
        # VÃ©rifier l'existence
        file_info = dbutils.fs.ls(zip_path)
        context.zip_exists = True
        print(f"âœ… Fichier trouvÃ©: {zip_path}")
        
    except Exception as e:
        context.zip_exists = False
        print(f"âš ï¸  Fichier non trouvÃ©: {zip_path}")
        raise AssertionError(f"Le fichier ZIP n'existe pas: {zip_path}")


@given('un fichier ZIP n\'existe pas Ã  "{zip_path}"')
def step_zip_not_exists(context, zip_path):
    """S'assurer qu'un fichier ZIP n'existe PAS"""
    context.zip_path = zip_path
    
    try:
        from pyspark.dbutils import DBUtils
        dbutils = DBUtils(context.spark)
        
        # VÃ©rifier que le fichier n'existe PAS
        file_info = dbutils.fs.ls(zip_path)
        raise AssertionError(f"âŒ Le fichier existe alors qu'il ne devrait pas: {zip_path}")
        
    except Exception as e:
        # C'est normal qu'il n'existe pas
        context.zip_exists = False
        print(f"âœ… Confirmation: fichier n'existe pas: {zip_path}")


@given('un fichier ZIP corrompu existe Ã  "{zip_path}"')
def step_corrupt_zip_exists(context, zip_path):
    """CrÃ©er ou utiliser un fichier ZIP corrompu"""
    context.zip_path = zip_path
    context.zip_exists = True
    context.is_corrupt = True
    
    # Note: Dans un vrai test, vous crÃ©eriez un fichier corrompu ici
    print(f"âš ï¸  Fichier corrompu: {zip_path}")


# ==========================================
# WHEN STEPS
# ==========================================

@when('je dÃ©compresse le fichier vers "{extract_path}"')
def step_unzip_to_path(context, extract_path):
    """DÃ©compresser le fichier ZIP"""
    context.extract_path = extract_path
    
    try:
        from pyspark.dbutils import DBUtils
        dbutils = DBUtils(context.spark)
        
        print(f"ğŸ”“ DÃ©compression: {context.zip_path}")
        print(f"ğŸ“‚ Destination: {extract_path}")
        
        # DÃ©compresser
        result = dbutils.fs.unzip(context.zip_path, extract_path)
        
        context.unzip_success = True
        context.unzip_error = None
        context.unzip_result = result
        
        print(f"âœ… DÃ©compression rÃ©ussie")
        
    except Exception as e:
        context.unzip_success = False
        context.unzip_error = e
        context.error_message = str(e)
        
        print(f"âŒ Erreur dÃ©compression: {e}")
        traceback.print_exc()


@when('je tente de dÃ©compresser le fichier')
def step_attempt_unzip(context):
    """Tenter de dÃ©compresser (peut Ã©chouer)"""
    context.extract_path = "/tmp/test_extract/"
    
    try:
        from pyspark.dbutils import DBUtils
        dbutils = DBUtils(context.spark)
        
        print(f"ğŸ”“ Tentative dÃ©compression: {context.zip_path}")
        
        # Tenter de dÃ©compresser
        result = dbutils.fs.unzip(context.zip_path, context.extract_path)
        
        context.unzip_success = True
        context.unzip_error = None
        
        print(f"âœ… DÃ©compression rÃ©ussie (inattendu)")
        
    except Exception as e:
        context.unzip_success = False
        context.unzip_error = e
        context.error_message = str(e).lower()
        context.error_type = type(e).__name__
        
        print(f"âŒ Erreur attendue: {type(e).__name__}")


# ==========================================
# THEN STEPS
# ==========================================

@then('la dÃ©compression doit rÃ©ussir')
def step_verify_unzip_success(context):
    """VÃ©rifier que la dÃ©compression a rÃ©ussi"""
    assert context.unzip_success is True, \
        f"âŒ La dÃ©compression a Ã©chouÃ©: {context.unzip_error}"
    
    print("âœ… DÃ©compression validÃ©e")


@then('les fichiers extraits doivent exister dans le dossier de destination')
def step_verify_files_exist(context):
    """VÃ©rifier que des fichiers existent dans le dossier"""
    try:
        from pyspark.dbutils import DBUtils
        dbutils = DBUtils(context.spark)
        
        # Lister les fichiers extraits
        files = dbutils.fs.ls(context.extract_path)
        
        assert len(files) > 0, \
            f"âŒ Aucun fichier trouvÃ© dans {context.extract_path}"
        
        print(f"âœ… {len(files)} fichier(s) extrait(s)")
        for file in files:
            print(f"   - {file.name}")
        
    except Exception as e:
        raise AssertionError(f"âŒ Impossible de lister les fichiers: {e}")


@then('au moins {min_count:d} fichier doit Ãªtre extrait')
def step_verify_min_files(context, min_count):
    """VÃ©rifier le nombre minimum de fichiers extraits"""
    try:
        from pyspark.dbutils import DBUtils
        dbutils = DBUtils(context.spark)
        
        files = dbutils.fs.ls(context.extract_path)
        actual_count = len(files)
        
        assert actual_count >= min_count, \
            f"âŒ Attendu au moins {min_count} fichier(s), trouvÃ© {actual_count}"
        
        print(f"âœ… {actual_count} fichier(s) extrait(s) (>= {min_count})")
        
    except Exception as e:
        raise AssertionError(f"âŒ Erreur comptage fichiers: {e}")


@then('une erreur doit Ãªtre levÃ©e')
def step_verify_error_raised(context):
    """VÃ©rifier qu'une erreur a bien Ã©tÃ© levÃ©e"""
    assert context.unzip_success is False, \
        "âŒ Aucune erreur levÃ©e alors qu'on en attendait une"
    
    assert context.unzip_error is not None, \
        "âŒ Aucune erreur enregistrÃ©e"
    
    print(f"âœ… Erreur levÃ©e comme attendu: {context.error_type}")


@then('le message d\'erreur doit contenir "{expected_text}"')
def step_verify_error_contains(context, expected_text):
    """VÃ©rifier que le message d'erreur contient un texte"""
    assert context.unzip_error is not None, \
        "âŒ Aucune erreur pour vÃ©rifier le message"
    
    error_msg = context.error_message.lower()
    expected = expected_text.lower()
    
    assert expected in error_msg, \
        f"âŒ Message d'erreur ne contient pas '{expected_text}'\n" \
        f"   Message rÃ©el: {context.error_message}"
    
    print(f"âœ… Message d'erreur contient bien '{expected_text}'")


@then('le message d\'erreur doit contenir "{text1}" ou "{text2}"')
def step_verify_error_contains_either(context, text1, text2):
    """VÃ©rifier que le message contient l'un des deux textes"""
    assert context.unzip_error is not None, \
        "âŒ Aucune erreur pour vÃ©rifier le message"
    
    error_msg = context.error_message.lower()
    t1 = text1.lower()
    t2 = text2.lower()
    
    assert (t1 in error_msg) or (t2 in error_msg), \
        f"âŒ Message d'erreur ne contient ni '{text1}' ni '{text2}'\n" \
        f"   Message rÃ©el: {context.error_message}"
    
    print(f"âœ… Message d'erreur vÃ©rifiÃ©")
```

---

## âš™ï¸ **3. FICHIER : `features/steps/environment.py`**

```python
# features/steps/environment.py

"""
Configuration de l'environnement Behave pour les tests BDD
"""

from pyspark.sql import SparkSession
import sys


def before_all(context):
    """
    Setup global AVANT tous les tests
    """
    print("\n" + "=" * 80)
    print("ğŸš€ INITIALISATION ENVIRONNEMENT DE TEST BDD")
    print("=" * 80)
    
    # Initialiser Spark
    try:
        context.spark = SparkSession.builder \
            .appName("WAX_BDD_Tests_Unzip") \
            .config("spark.sql.shuffle.partitions", "2") \
            .getOrCreate()
        
        print(f"âœ… Spark {context.spark.version} initialisÃ©")
        print(f"ğŸ“ Spark Master: {context.spark.sparkContext.master}")
        
    except Exception as e:
        print(f"âŒ Erreur initialisation Spark: {e}")
        sys.exit(1)
    
    # Configurer les chemins
    context.catalog = "abu_catalog"
    context.schema = "gdp_poc_dev"
    context.volume = "externalvolumetest"
    
    print(f"ğŸ“‚ Catalog: {context.catalog}")
    print(f"ğŸ“‚ Schema: {context.schema}")
    print(f"ğŸ“‚ Volume: {context.volume}")
    print("=" * 80 + "\n")


def after_all(context):
    """
    Cleanup APRÃˆS tous les tests
    """
    print("\n" + "=" * 80)
    print("ğŸ§¹ NETTOYAGE ENVIRONNEMENT DE TEST")
    print("=" * 80)
    
    if hasattr(context, 'spark') and context.spark is not None:
        try:
            context.spark.stop()
            print("âœ… Spark arrÃªtÃ© proprement")
        except Exception as e:
            print(f"âš ï¸  Erreur arrÃªt Spark: {e}")
    
    print("=" * 80 + "\n")


def before_scenario(context, scenario):
    """
    Setup AVANT chaque scÃ©nario
    """
    print("\n" + "-" * 80)
    print(f"ğŸ“‹ SCÃ‰NARIO: {scenario.name}")
    print(f"ğŸ·ï¸  Tags: {', '.join(scenario.tags) if scenario.tags else 'Aucun'}")
    print("-" * 80)
    
    # RÃ©initialiser les variables de contexte
    context.zip_path = None
    context.extract_path = None
    context.zip_exists = False
    context.unzip_success = False
    context.unzip_error = None
    context.error_message = None
    context.error_type = None


def after_scenario(context, scenario):
    """
    Cleanup APRÃˆS chaque scÃ©nario
    """
    if scenario.status == "failed":
        print(f"\nâŒ SCÃ‰NARIO Ã‰CHOUÃ‰: {scenario.name}")
        
        # Afficher des infos de debug
        if hasattr(context, 'unzip_error') and context.unzip_error:
            print(f"\nğŸ” DÃ‰TAILS DE L'ERREUR:")
            print(f"   Type: {type(context.unzip_error).__name__}")
            print(f"   Message: {context.unzip_error}")
    else:
        print(f"\nâœ… SCÃ‰NARIO RÃ‰USSI: {scenario.name}")
    
    print("-" * 80)


def before_feature(context, feature):
    """
    Setup AVANT chaque feature
    """
    print("\n" + "=" * 80)
    print(f"ğŸ¯ FEATURE: {feature.name}")
    print("=" * 80)


def after_feature(context, feature):
    """
    Cleanup APRÃˆS chaque feature
    """
    passed = sum(1 for s in feature.scenarios if s.status == "passed")
    failed = sum(1 for s in feature.scenarios if s.status == "failed")
    total = len(feature.scenarios)
    
    print("\n" + "=" * 80)
    print(f"ğŸ“Š RÃ‰SULTATS FEATURE: {feature.name}")
    print(f"   âœ… RÃ©ussis: {passed}/{total}")
    print(f"   âŒ Ã‰chouÃ©s: {failed}/{total}")
    print("=" * 80)
```

---

## ğŸ”§ **4. FICHIER : `features/steps/__init__.py`**

```python
# features/steps/__init__.py

"""
Package des step definitions pour Behave
"""

# Ce fichier peut rester vide, il indique juste que c'est un package Python
```

---

## ğŸ¯ **5. FICHIER : `waxng/run_bdd_tests.py`**

```python
# waxng/run_bdd_tests.py

"""
Point d'entrÃ©e pour exÃ©cuter les tests BDD sur Databricks
"""

import sys
import os
import subprocess
from pathlib import Path


def main():
    """
    ExÃ©cute les tests BDD avec Behave
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª EXÃ‰CUTION DES TESTS BDD - MODULE UNZIP")
    print("=" * 80 + "\n")
    
    # Trouver le rÃ©pertoire des features
    current_dir = Path(__file__).parent.parent  # Remonte au dossier waxng/
    features_path = current_dir / "features"
    
    if not features_path.exists():
        print(f"âŒ Dossier features introuvable: {features_path}")
        print(f"ğŸ“‚ RÃ©pertoire actuel: {current_dir}")
        sys.exit(1)
    
    print(f"ğŸ“‚ Features path: {features_path}")
    print(f"ğŸ“‚ Current dir: {current_dir}")
    
    # Configurer l'environnement
    env = os.environ.copy()
    env['PYTHONPATH'] = str(current_dir)
    
    # Arguments Behave
    behave_args = [
        sys.executable, "-m", "behave",
        str(features_path / "unzip.feature"),  # Test uniquement unzip.feature
        "--format", "pretty",
        "--no-capture",
        "--tags", "~@skip",  # Exclure les tests @skip
        # "--tags", "@smoke",  # DÃ©commenter pour n'exÃ©cuter que les tests @smoke
        "--color",  # Couleurs dans le terminal
    ]
    
    print(f"ğŸš€ Commande: {' '.join(behave_args)}\n")
    
    # ExÃ©cuter Behave
    try:
        result = subprocess.run(
            behave_args,
            env=env,
            cwd=str(current_dir),
            capture_output=False,  # Afficher directement la sortie
            text=True
        )
        
        # VÃ©rifier le rÃ©sultat
        print("\n" + "=" * 80)
        if result.returncode == 0:
            print("âœ… TOUS LES TESTS BDD ONT RÃ‰USSI")
            print("=" * 80 + "\n")
            sys.exit(0)
        else:
            print("âŒ CERTAINS TESTS BDD ONT Ã‰CHOUÃ‰")
            print(f"Code retour: {result.returncode}")
            print("=" * 80 + "\n")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ Erreur lors de l'exÃ©cution des tests: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
```

---

## ğŸ“¦ **6. MODIFIER `setup.py`**

```python
# setup.py

from setuptools import setup, find_packages

setup(
    name="waxng",
    version="1.0.0",
    packages=find_packages(),
    
    # Points d'entrÃ©e
    entry_points={
        'console_scripts': [
            'unzip_module=waxng.unzip_module:main',
            'autoloader_module=waxng.autoloader_module:main',
            'main=waxng.main:main',
            'run_bdd_tests=waxng.run_bdd_tests:main',  # âœ… Point d'entrÃ©e tests
        ],
    },
    
    # âœ… Inclure les fichiers .feature
    package_data={
        'waxng': [],
    },
    include_package_data=True,
    
    # Fichiers Ã  inclure via MANIFEST.in
    # (voir ci-dessous)
    
    install_requires=[
        'pyspark>=3.0.0',
        'pandas',
        'openpyxl',
    ],
    
    # DÃ©pendances pour les tests
    extras_require={
        'test': [
            'behave>=1.2.6',
            'pytest>=7.0.0',
        ],
    },
)
```

---

## ğŸ“„ **7. CRÃ‰ER `MANIFEST.in`**

```
# MANIFEST.in

# Inclure les features et steps
recursive-include features *.feature
recursive-include features/steps *.py
```

---

## ğŸš€ **8. EXÃ‰CUTION LOCALE**

```bash
# 1. Installer Behave
pip install behave

# 2. Depuis le dossier racine waxng/
cd /path/to/waxng

# 3. ExÃ©cuter tous les tests
behave features/

# 4. ExÃ©cuter uniquement unzip.feature
behave features/unzip.feature

# 5. ExÃ©cuter uniquement les tests @smoke
behave features/unzip.feature --tags=@smoke

# 6. ExÃ©cuter SAUF les tests @negative
behave features/unzip.feature --tags=~@negative

# 7. Mode verbose avec couleurs
behave features/unzip.feature -v --color

# 8. GÃ©nÃ©rer un rapport HTML
behave features/unzip.feature --format=html --outfile=report.html
```

---

## ğŸ—ï¸ **9. BUILDER LE PACKAGE**

```bash
# 1. CrÃ©er le wheel
python setup.py bdist_wheel

# 2. VÃ©rifier le contenu du wheel
unzip -l dist/waxng-1.0.0-py3-none-any.whl | grep features

# Vous devriez voir:
#   features/unzip.feature
#   features/steps/environment.py
#   features/steps/unzip_steps.py
#   features/steps/__init__.py
```

---

## â˜ï¸ **10. DÃ‰PLOYER SUR DATABRICKS**

```bash
# 1. DÃ©ployer le bundle
databricks bundle deploy --target dev

# 2. Lancer les tests manuellement
databricks jobs run-now --job-name "waxng_job"

# 3. Ou crÃ©er un job dÃ©diÃ© aux tests
# Voir le fichier waxng_job.yml modifiÃ© ci-dessous
```

---

## ğŸ“Š **RÃ‰SULTAT ATTENDU**

Quand vous exÃ©cutez `behave features/unzip.feature`, vous devriez voir :

```
Feature: DÃ©compression de fichiers ZIP

  Background:
    Given Spark est disponible               âœ… passed
    And les chemins de base sont configurÃ©s  âœ… passed

  @smoke @unzip
  Scenario: DÃ©compresser un fichier ZIP valide
    Given un fichier ZIP existe Ã  ...        âœ… passed
    When je dÃ©compresse le fichier vers ...  âœ… passed
    Then la dÃ©compression doit rÃ©ussir       âœ… passed
    And les fichiers extraits doivent ...    âœ… passed

  @unzip
  Scenario: Compter les fichiers extraits
    ...                                       âœ… passed

  @unzip @negative
  Scenario: GÃ©rer un fichier ZIP inexistant
    ...                                       âœ… passed

4 scenarios (4 passed)
12 steps (12 passed)
```

---

**Tous les fichiers sont complets et prÃªts Ã  utiliser ! Besoin d'aide pour l'exÃ©cution ?** ğŸš€
