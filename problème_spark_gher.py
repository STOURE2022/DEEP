# ğŸ”´ **ERREUR : Initialisation Spark sur Databricks**

## **PROBLÃˆME**

Sur Databricks, vous ne pouvez PAS crÃ©er une nouvelle SparkSession avec `SparkSession.builder`. Spark est **dÃ©jÃ  disponible** !

---

## âœ… **SOLUTION : Modifier `environment.py`**

### **Remplacer COMPLÃˆTEMENT le fichier `features/steps/environment.py` :**

```python
# features/steps/environment.py

"""
Configuration de l'environnement Behave pour les tests BDD
Compatible Databricks et exÃ©cution locale
"""

from pyspark.sql import SparkSession
import sys
import os


def _get_spark_session():
    """
    Obtenir la session Spark selon l'environnement
    
    Returns:
        SparkSession: Session Spark active
    """
    # âœ… Sur Databricks : Utiliser la session existante
    try:
        # VÃ©rifier si on est sur Databricks
        if 'DATABRICKS_RUNTIME_VERSION' in os.environ:
            print("ğŸ¯ Environnement dÃ©tectÃ©: Databricks")
            
            # RÃ©cupÃ©rer la session Spark existante
            spark = SparkSession.getActiveSession()
            
            if spark is None:
                # Si pas de session active, en crÃ©er une (ne devrait pas arriver sur Databricks)
                spark = SparkSession.builder.getOrCreate()
            
            return spark
        
        else:
            # âœ… ExÃ©cution locale : CrÃ©er une session
            print("ğŸ¯ Environnement dÃ©tectÃ©: Local")
            
            spark = SparkSession.builder \
                .appName("WAX_BDD_Tests_Local") \
                .config("spark.sql.shuffle.partitions", "2") \
                .config("spark.driver.memory", "2g") \
                .master("local[*]") \
                .getOrCreate()
            
            return spark
            
    except Exception as e:
        print(f"âš ï¸  Erreur dÃ©tection environnement: {e}")
        
        # Fallback : essayer de rÃ©cupÃ©rer session active
        spark = SparkSession.getActiveSession()
        if spark:
            return spark
        
        # Dernier recours : crÃ©er une session locale
        return SparkSession.builder \
            .appName("WAX_BDD_Tests") \
            .getOrCreate()


def before_all(context):
    """
    Setup global AVANT tous les tests
    """
    print("\n" + "=" * 80)
    print("ğŸš€ INITIALISATION ENVIRONNEMENT DE TEST BDD")
    print("=" * 80)
    
    # Obtenir Spark selon l'environnement
    try:
        context.spark = _get_spark_session()
        
        if context.spark is None:
            raise RuntimeError("Impossible d'obtenir une session Spark")
        
        print(f"âœ… Spark {context.spark.version} disponible")
        print(f"ğŸ“ Spark Master: {context.spark.sparkContext.master}")
        
        # VÃ©rifier si dbutils est disponible (Databricks)
        try:
            from pyspark.dbutils import DBUtils
            context.dbutils = DBUtils(context.spark)
            print("âœ… dbutils disponible")
        except Exception:
            context.dbutils = None
            print("âš ï¸  dbutils non disponible (mode local)")
        
    except Exception as e:
        print(f"âŒ Erreur initialisation Spark: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Configurer les chemins
    context.catalog = "abu_catalog"
    context.schema = "gdp_poc_dev"
    context.volume = "externalvolumetest"
    
    print(f"ğŸ“‚ Catalog: {context.catalog}")
    print(f"ğŸ“‚ Schema: {context.schema}")
    print(f"ğŸ“‚ Volume: {context.volume}")
    print("=" * 80 + "\n")


def after_all(context):
    """
    Cleanup APRÃˆS tous les tests
    """
    print("\n" + "=" * 80)
    print("ğŸ§¹ NETTOYAGE ENVIRONNEMENT DE TEST")
    print("=" * 80)
    
    # Sur Databricks, NE PAS arrÃªter Spark !
    if 'DATABRICKS_RUNTIME_VERSION' not in os.environ:
        if hasattr(context, 'spark') and context.spark is not None:
            try:
                context.spark.stop()
                print("âœ… Spark arrÃªtÃ© proprement (mode local)")
            except Exception as e:
                print(f"âš ï¸  Erreur arrÃªt Spark: {e}")
    else:
        print("â„¹ï¸  Session Spark conservÃ©e (Databricks)")
    
    print("=" * 80 + "\n")


def before_scenario(context, scenario):
    """
    Setup AVANT chaque scÃ©nario
    """
    print("\n" + "-" * 80)
    print(f"ğŸ“‹ SCÃ‰NARIO: {scenario.name}")
    print(f"ğŸ·ï¸  Tags: {', '.join(scenario.tags) if scenario.tags else 'Aucun'}")
    print("-" * 80)
    
    # RÃ©initialiser les variables de contexte
    context.zip_path = None
    context.extract_path = None
    context.zip_exists = False
    context.unzip_success = False
    context.unzip_error = None
    context.error_message = None
    context.error_type = None


def after_scenario(context, scenario):
    """
    Cleanup APRÃˆS chaque scÃ©nario
    """
    if scenario.status == "failed":
        print(f"\nâŒ SCÃ‰NARIO Ã‰CHOUÃ‰: {scenario.name}")
        
        # Afficher des infos de debug
        if hasattr(context, 'unzip_error') and context.unzip_error:
            print(f"\nğŸ” DÃ‰TAILS DE L'ERREUR:")
            print(f"   Type: {type(context.unzip_error).__name__}")
            print(f"   Message: {context.unzip_error}")
    else:
        print(f"\nâœ… SCÃ‰NARIO RÃ‰USSI: {scenario.name}")
    
    print("-" * 80)


def before_feature(context, feature):
    """
    Setup AVANT chaque feature
    """
    print("\n" + "=" * 80)
    print(f"ğŸ¯ FEATURE: {feature.name}")
    print("=" * 80)


def after_feature(context, feature):
    """
    Cleanup APRÃˆS chaque feature
    """
    passed = sum(1 for s in feature.scenarios if s.status == "passed")
    failed = sum(1 for s in feature.scenarios if s.status == "failed")
    skipped = sum(1 for s in feature.scenarios if s.status == "skipped")
    total = len(feature.scenarios)
    
    print("\n" + "=" * 80)
    print(f"ğŸ“Š RÃ‰SULTATS FEATURE: {feature.name}")
    print(f"   âœ… RÃ©ussis: {passed}/{total}")
    
    if failed > 0:
        print(f"   âŒ Ã‰chouÃ©s: {failed}/{total}")
    
    if skipped > 0:
        print(f"   â­ï¸  IgnorÃ©s: {skipped}/{total}")
    
    print("=" * 80)
```

---

## ğŸ”§ **AUSSI : VÃ©rifier `run_bdd_tests.py`**

Assurez-vous que le script n'essaie pas de crÃ©er Spark non plus :

```python
# waxng/run_bdd_tests.py

"""
Point d'entrÃ©e pour exÃ©cuter les tests BDD sur Databricks
"""

import sys
import os
import subprocess
from pathlib import Path


def main():
    """
    ExÃ©cute les tests BDD avec Behave
    """
    print("\n" + "=" * 80)
    print("ğŸ§ª EXÃ‰CUTION DES TESTS BDD - MODULE UNZIP")
    print("=" * 80 + "\n")
    
    # Trouver le rÃ©pertoire des features
    # Sur Databricks avec wheel, le package est dans site-packages
    try:
        import waxng
        waxng_path = Path(waxng.__file__).parent
        features_path = waxng_path.parent / "features"
        
        # Si features n'est pas trouvÃ©, essayer autre chemin
        if not features_path.exists():
            features_path = waxng_path / "features"
        
    except ImportError:
        # Fallback : utiliser le rÃ©pertoire courant
        current_dir = Path(__file__).parent.parent
        features_path = current_dir / "features"
    
    if not features_path.exists():
        print(f"âŒ Dossier features introuvable: {features_path}")
        print(f"ğŸ“‚ Chemins essayÃ©s:")
        print(f"   - {waxng_path.parent / 'features'}")
        print(f"   - {waxng_path / 'features'}")
        sys.exit(1)
    
    print(f"ğŸ“‚ Features path: {features_path}")
    
    # Configurer l'environnement
    env = os.environ.copy()
    
    # Arguments Behave
    behave_args = [
        sys.executable, "-m", "behave",
        str(features_path),  # ExÃ©cuter tous les .feature
        "--format", "pretty",
        "--no-capture",
        "--tags", "~@skip",
        "--color",
    ]
    
    print(f"ğŸš€ Commande: {' '.join(behave_args)}\n")
    
    # ExÃ©cuter Behave
    try:
        result = subprocess.run(
            behave_args,
            env=env,
            capture_output=False,
            text=True
        )
        
        # VÃ©rifier le rÃ©sultat
        print("\n" + "=" * 80)
        if result.returncode == 0:
            print("âœ… TOUS LES TESTS BDD ONT RÃ‰USSI")
            print("=" * 80 + "\n")
            sys.exit(0)
        else:
            print("âŒ CERTAINS TESTS BDD ONT Ã‰CHOUÃ‰")
            print(f"Code retour: {result.returncode}")
            print("=" * 80 + "\n")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ Erreur lors de l'exÃ©cution des tests: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
```

---

## ğŸ“¦ **RE-BUILDER LE PACKAGE**

```bash
# 1. Nettoyer les anciens builds
rm -rf dist/ build/ *.egg-info

# 2. Re-builder le wheel
python setup.py bdist_wheel

# 3. VÃ©rifier le contenu
unzip -l dist/waxng-1.0.0-py3-none-any.whl | grep -E "(features|environment)"
```

---

## â˜ï¸ **RE-DÃ‰PLOYER SUR DATABRICKS**

```bash
# 1. Re-dÃ©ployer
databricks bundle deploy --target dev

# 2. Relancer les tests
databricks jobs run-now --job-name "waxng_job"
```

---

## ğŸ“Š **RÃ‰SULTAT ATTENDU**

Maintenant vous devriez voir :

```
ğŸ¯ Environnement dÃ©tectÃ©: Databricks
âœ… Spark 14.3.x disponible
âœ… dbutils disponible
ğŸ“‚ Catalog: abu_catalog
ğŸ“‚ Schema: gdp_poc_dev
ğŸ“‚ Volume: externalvolumetest

Feature: DÃ©compression de fichiers ZIP
  ...
  âœ… 0 features passed, 0 failed, 0 skipped
```

---

## ğŸ” **ALTERNATIVE : Tester SANS Spark**

Si vous voulez tester la logique sans Databricks, crÃ©ez un fichier `test_mock.py` :

```python
# tests/test_unzip_mock.py

"""Tests unitaires sans Spark pour la logique mÃ©tier"""

import pytest
from unittest.mock import Mock, patch

def test_parse_tolerance():
    """Tester parse_tolerance sans Spark"""
    from waxng.utils import parse_tolerance
    
    assert parse_tolerance("10%", 100) == 10
    assert parse_tolerance("5", 100) == 0.05
    assert parse_tolerance("20%", 1000) == 200

def test_unzip_logic_mock():
    """Tester la logique unzip avec des mocks"""
    # Mock dbutils
    mock_dbutils = Mock()
    mock_dbutils.fs.unzip.return_value = True
    
    # Votre logique mÃ©tier ici
    result = mock_dbutils.fs.unzip("/path/to/file.zip", "/extract/")
    
    assert result is True
    mock_dbutils.fs.unzip.assert_called_once()
```

---

**Le problÃ¨me principal Ã©tait l'initialisation de Spark. Sur Databricks, il faut TOUJOURS utiliser la session existante ! ğŸ¯**
